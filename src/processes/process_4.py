# src/processes/process_4.py

import torch
import os
import logging
from datetime import datetime
from torch.utils.data import DataLoader

# Modules
from src.modules.ner_evaluator import Evaluator
from src.models.ner_roberta import RobertaNerModel

# Database
from src.database.connection import db_manager
from src.database import crud

# Utils
from src.utils.common import ensure_dir, save_logs_to_csv
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import math

# test
import difflib

def run_process_4(config: dict, context: dict):
    """
    [Process 4] ëª¨ë¸ ë³´ì™„ ì¶”ë¡  ë° Hybrid ê²€ì¦ í”„ë¡œì„¸ìŠ¤
    
    1. ê·œì¹™(ì‚¬ì „/Regex)ì´ ì°¾ì€ ê²°ê³¼ë¥¼ DBì—ì„œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ì— ë§¤í•‘.
    2. í•™ìŠµëœ Best Modelë¡œ ì „ì²´ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•´ ì¶”ë¡  ìˆ˜í–‰.
    3. ëª¨ë¸ ê²°ê³¼ì™€ ê·œì¹™ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ìœ í˜• ë¶„ë¥˜ ë° í†µê³„ ì‚°ì¶œ:
       - Double Check: ê·œì¹™ë„ ì°¾ê³  ëª¨ë¸ë„ ì°¾ìŒ (ì‹ ë¢°ë„ ë†’ìŒ)
       - Model Complement: ê·œì¹™ì€ ëª» ì°¾ì•˜ëŠ”ë° ëª¨ë¸ì´ ì°¾ìŒ (ëª¨ë¸ì˜ ê¸°ì—¬ë„)
       - Rule Only: ê·œì¹™ì€ ì°¾ì•˜ëŠ”ë° ëª¨ë¸ì€ ëª» ì°¾ìŒ (ëª¨ë¸ì˜ í•œê³„)
    4. ë¶„ì„ ê²°ê³¼ ë° ë¡œê·¸ DB ì €ì¥ & CSV ì¶”ì¶œ.
    """
    
    # ==============================================================================
    # [Step 1] ì„¤ì • ë° ë¡œê±° ì´ˆê¸°í™”
    # ==============================================================================
    experiment_code = context['experiment_code']
    device = context['device']
    preprocessor = context['preprocessor']
    
    path_conf = config['path']
    train_conf = config['train']

    logger = logging.getLogger(experiment_code)
    logger.info(f"ğŸš€ [Process 4] Start Hybrid Inference & Analysis")

    # ==============================================================================
    # [Step 2] ê·œì¹™ ê¸°ë°˜ íƒì§€ ê²°ê³¼ ë¡œë“œ (Process 2 & 3)
    # ==============================================================================
    logger.info("Loading Rule-based detection results from DB...")
    
    rule_hits = {}
    
    with db_manager.get_db() as session:
        for proc_code in ["process_2", "process_3"]:
            logs = crud.get_inference_sentences(session, experiment_code, proc_code, 1)
            for log in logs:
                sid = log['sentence_id']
                if sid not in rule_hits:
                    rule_hits[sid] = {}
                
                res = log.get('sentence_inference_result', {})
                results_list = res.get('inference_results', [])
                
                for r in results_list:
                    if r.get('match_result') in ['hit', 'prediction']:
                        word = r['word']
                        label = r['label']
                        rule_hits[sid][word] = label

    logger.info(f"Loaded rule hits for {len(rule_hits)} sentences.")

    # ==============================================================================
    # [Step 3] Best Model ë¡œë“œ
    # ==============================================================================
    logger.info("Loading Best Model from Checkpoint...")
    
    encoder = context['model'].encoder 
    num_labels = len(preprocessor.ner_label2id)
    
    best_model = RobertaNerModel(
        encoder=encoder,
        num_classes=num_labels,
        use_focal=False 
    ).to(device)
    
    ckpt_path = os.path.join(
        path_conf['checkpoint_dir'], experiment_code, f"{experiment_code}_epoch_{context['best_epoch']}.pt"
    )
    
    if os.path.exists(ckpt_path):
        best_model.load_state_dict(torch.load(ckpt_path, map_location=device))
        logger.info(f"âœ… Loaded weights from {ckpt_path}")
    else:
        logger.warning(f"âš ï¸ Checkpoint not found at {ckpt_path}. Using current model state.")
        best_model = context['model']

    # ==============================================================================
    # [Step 4] ì¶”ë¡  ë° ë¹„êµ ë¶„ì„ (Hybrid Logic)
    # ==============================================================================
    evaluator = Evaluator(
        best_model, 
        device, 
        preprocessor.tokenizer, 
        preprocessor.ner_id2label 
    )

    result = evaluator.evaluate(context['test_loader']) # ì´ê±° mode="test"ë¥¼ ì—†ì• ì•¼ í•  ê²ƒ ê°™ì€ë°...? ã…‡ã…‡ ì—†ì•° ã…‹
    raw_logs = result['logs']

    stats = {
        "double_check": 0, "model_complement": 0, "rule_only": 0, "total_model_detected": 0
    }

    processed_logs = []

    for log in raw_logs:
        sid = log['sentence_id']
        model_results = log['sentence_inference_result']['inference_results']

        rule_findings = rule_hits.get(sid, {}).copy() 
        
        # 1. ëª¨ë¸ íƒì§€ ê²°ê³¼ ìˆœíšŒ
        for entity in model_results:
            word = entity['word']
            if word in rule_findings:
                entity['hybrid_status'] = "Double Check"
                stats['double_check'] += 1
                rule_findings.pop(word, None)
            else:
                entity['hybrid_status'] = "Model Complement"
                stats['model_complement'] += 1
            stats['total_model_detected'] += 1
            
        # 2. Rule Only ê³„ì‚°
        for r_word, r_label in rule_findings.items():
            stats['rule_only'] += 1
            model_results.append({
                "word": r_word,
                "label": r_label,
                "start": -1, 
                "end": -1,
                "hybrid_status": "Rule Only (Model Missed)"
            })
        
        log['sentence_inference_result']['inference_results'] = model_results
        log['sentence_inference_result']['entity_count'] = len(model_results)
        processed_logs.append(log)

    # ë¹„ìœ¨ ê³„ì‚°
    total_detections = stats['double_check'] + stats['model_complement'] + stats['rule_only']
    if total_detections > 0:
        stats['ratio_double_check'] = round(stats['double_check'] / total_detections, 4)
        stats['ratio_complement'] = round(stats['model_complement'] / total_detections, 4)
        stats['ratio_rule_only'] = round(stats['rule_only'] / total_detections, 4)
    else:
        stats.update({'ratio_double_check': 0, 'ratio_complement': 0, 'ratio_rule_only': 0})

    logger.info(f"ğŸ“Š Hybrid Analysis Result: {stats}")

    # ==============================================================================
    # [Step 5] DB ì €ì¥ ë° CSV ì¶”ì¶œ    ### ì—¬ê¸° ë ˆì´ì–´ ì¢€ ë¬´ë„ˆì ¸ìˆìŒ;;; ã…  ì‹œë°”
    # ==============================================================================
    
    # CSV ì €ì¥ ê²½ë¡œ ìƒì„±
    log_save_dir = os.path.join(path_conf['log_dir'], experiment_code)
    ensure_dir(log_save_dir)

    with db_manager.get_db() as session:
        # 5-1. ê²°ê³¼ ìš”ì•½ ì €ì¥
        crud.create_process_result(session, {
            "experiment_code": experiment_code,
            "process_code": "process_4", 
            "process_epoch": 1,
            "process_start_time": datetime.now(), 
            "process_end_time": result.get('end_time', datetime.now()),
            "process_duration": result['metrics'].get('duration', 0.0),
            "process_results": {
                "hybrid_stats": stats,
                "base_metrics": result['metrics']
            }
        })

        # 5-2. ë¬¸ì¥ ë¡œê·¸ ì €ì¥ (Bulk Insert)
        # FK ì£¼ì…
        for log in processed_logs:
            log['experiment_code'] = experiment_code
            log['process_code'] = "process_4"
            log['process_epoch'] = 1
        
        crud.bulk_insert_inference_sentences(session, processed_logs)
        logger.info(f"Saved {len(processed_logs)} hybrid inference logs to DB.")
        
        # 5-3. ê° GTë¼ë²¨ì— ëŒ€í•˜ì—¬ Predëœ ë¼ë²¨ë“¤(with conf score)ì— ëŒ€í•œ plotê³¼ ì •íƒ ë¯¸íƒ ì˜¤íƒ ê°œìˆ˜ì™€ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ Plotì— ì¶”ê°€
        # key: GT label, value: PRED label, 
        final_results = {}
        for log in raw_logs:
            sentence_inference_result_list = log['sentence_inference_result']
            inferenced_results_list = sentence_inference_result_list['inference_results']
            token_comparison_list = sentence_inference_result_list['token_comparison']

            # [ìˆ˜ì •] ë”•ì…”ë„ˆë¦¬ ê°ì²´ ëŒ€ì‹  ê³ ìœ  í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            non_normal_token_comparison = {}
            for token_comparison in token_comparison_list:
                pred_entity = token_comparison['pred_entity']
                # ë‹¨ì–´ì™€ ì‹œì‘/ë ìœ„ì¹˜ë¥¼ ì¡°í•©í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±
                unique_key = f"{pred_entity['word']}_{pred_entity['start']}_{pred_entity['end']}"
                non_normal_token_comparison[unique_key] = token_comparison

            # í™•ì¸(ì¼ë°˜ì •ë³´)
            for token_comparison in token_comparison_list:
                if token_comparison['pred_label'] == "ì¼ë°˜ì •ë³´":
                    pred_result = token_comparison['pred_entity']
                    pred_result['score'] = 0.0
                    pred_result['sentence_id'] = sentence_inference_result_list['sentence_id']
                    pred_result['origin_sentence'] = sentence_inference_result_list['origin_sentence']
                    pred_result['gt_word'] = token_comparison['gt_entity']['word']

                    gt_label = token_comparison['gt_label']
                    if gt_label in final_results:
                        final_results[gt_label].append(pred_result)
                    else:
                        final_results[gt_label] = [pred_result]

            # í™•ì¸(ì¼ë°˜ì •ë³´ ì œì™¸)
            for inferenced_result in inferenced_results_list:
                # [ìˆ˜ì •] ì¡°íšŒí•  ë•Œë„ ë™ì¼í•œ ê·œì¹™ìœ¼ë¡œ í‚¤ ìƒì„±
                current_key = f"{inferenced_result['word']}_{inferenced_result['start']}_{inferenced_result['end']}"
                
                # ìƒì„±í•œ ë¬¸ìì—´ í‚¤ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ë” ì´ìƒ ì—ëŸ¬ ë°œìƒ X)
                if current_key in non_normal_token_comparison:
                    curr_comparison = non_normal_token_comparison[current_key]
                    inferenced_result['gt_word'] = curr_comparison['gt_entity']['word']
                    inferenced_result['score'] = curr_comparison['score']
                    inferenced_result['sentence_id'] = sentence_inference_result_list['sentence_id']
                    inferenced_result['origin_sentence'] = sentence_inference_result_list['origin_sentence']

                    gt_label = curr_comparison['gt_label']
                    if gt_label in final_results:
                        final_results[gt_label].append(inferenced_result)
                    else:
                        final_results[gt_label] = [inferenced_result]
                
                else:
                    # GTê°€ 'ì¼ë°˜ì •ë³´'ì¸ ê²½ìš°
                    inferenced_result['score'] = 0.0
                    inferenced_result['sentence_id'] = sentence_inference_result_list['sentence_id']
                    inferenced_result['origin_sentence'] = sentence_inference_result_list['origin_sentence']
                    inferenced_result['gt_word'] = 'NULL'

                    if "ì¼ë°˜ì •ë³´" in final_results:
                        final_results["ì¼ë°˜ì •ë³´"].append(inferenced_result)
                    else:
                        final_results["ì¼ë°˜ì •ë³´"] = [inferenced_result]

        # --- ë ˆì´ì–´ ì „í˜€ ì•ˆì§€í‚¤ì´ì´ì„ --- #
        # --- 999ë²ˆ ë„ë©”ì¸ z-score ë¶ˆëŸ¬ì˜¤ê¸° ---
        z_score = None

        with db_manager.get_db() as session:
            z_score = list(crud.get_dtm_by_domain(session, 999))

        # print('[debug] z_score\n\n', z_score)

        z_score_by_term = {}

        for data in z_score:
            if data['term'] in z_score_by_term and data['z_score'] < z_score_by_term[data['term']]:
                continue
            z_score_by_term[data['term']] = data['z_score']

        for gt_label, result_dict in final_results.items():
            for result in result_dict:
                gt_word = result['gt_word']
                pred_word = result['word']
                answer_bytes = difflib.SequenceMatcher(None, gt_word, pred_word)
                result['ratio_score'] = answer_bytes.ratio()
                result['z_score'] = z_score_by_term.get(gt_word, 0.0)
    
        for gt_label, results_list in final_results.items():
            # 1. 'ì¼ë°˜ì •ë³´' ê±´ë„ˆë›°ê¸°
            if gt_label == 'ì¼ë°˜ì •ë³´':
                continue
            
            # í•´ë‹¹ gt_labelì˜ ì „ì²´ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
            total_samples = len(results_list)
            if total_samples == 0:
                continue

            # 2. ê²°ê³¼ ë°ì´í„°ë¥¼ pred_label ë³„ë¡œ ê·¸ë£¹í™” (Subplot ìƒì„±ì„ ìœ„í•´)
            from collections import defaultdict
            grouped_data = defaultdict(list)
            for res in results_list:
                grouped_data[res['label']].append(res)

            num_subs = len(grouped_data)
            fig, axes = plt.subplots(num_subs, 1, figsize=(10, 2 * num_subs), squeeze=False)
            
            for idx, (pred_label, samples) in enumerate(grouped_data.items()):
                ax = axes[idx, 0]
                
                # ë°ì´í„° ì¶”ì¶œ
                z_scores = [min(s.get('z_score', 0.0), 4.0) for s in samples]
                ratio_scores = [s.get('ratio_score', 0.0) for s in samples]

                # ì´ìƒì¹˜ ìˆ˜ í™•ì¸
                outlier = sum(1 for s in samples if s.get('z_score', 0.0) > 4.0)

                # boxplot ê¸°ì¤€
                z_scores_arr = np.array(z_scores)
                p100, p75, p50, p25, p0 = np.percentile(z_scores_arr, [100, 75, 50, 25, 0])
                
                # í†µê³„ì¹˜ ê³„ì‚°
                count = len(samples)
                percentage = (count / total_samples) * 100
                z_avg = np.mean(z_scores)
                z_std = np.std(z_scores)
                
                # 3. ì‚°ì ë„ ê·¸ë¦¬ê¸°
                ax.scatter(z_scores, ratio_scores, alpha=0.6, edgecolors='w', label=f'Samples (n={count})')
                
                # 3. ê° ì§€ì ì— ìˆ˜ì§ ì ì„  ê·¸ë¦¬ê¸° (axvline)
                # ì¤‘ì•™ê°’ (50%) - ê°€ì¥ ì¤‘ìš”í•˜ë¯€ë¡œ ë¹¨ê°„ìƒ‰ ì‹¤ì„  í˜¹ì€ êµµì€ ì ì„ 
                ax.axvline(p50, color='red', linestyle='--', linewidth=1.5, alpha=0.8, label=f'Median ({p50:.2f})')
                
                # 25% ë° 75% ì§€ì  - ì˜¤ë Œì§€ìƒ‰ ì ì„ 
                ax.axvline(p25, color='orange', linestyle=':', alpha=0.6, label=f'Q1/Q3 ({p25:.2f}, {p75:.2f})')
                ax.axvline(p75, color='orange', linestyle=':', alpha=0.6)
                
                # 0% ë° 100% ì§€ì  (ìµœì†Ÿê°’/ìµœëŒ“ê°’) - íšŒìƒ‰ ì•„ì£¼ ì—°í•œ ì ì„ 
                ax.axvline(p0, color='green', linestyle='-.', alpha=0.4, label='Min/Max')
                ax.axvline(p100, color='green', linestyle='-.', alpha=0.4)

                # 4. ë²”ë¡€ ì¶”ê°€ (ì„ ì˜ ì˜ë¯¸ë¥¼ ì•Œê¸° ìœ„í•´)
                ax.legend(loc='upper right', fontsize='small')

                # 5. (ì„ íƒì‚¬í•­) xì¶• ë²”ìœ„ ìµœì í™”
                # 4.0ìœ¼ë¡œ clipping í•˜ì…¨ë‹¤ë©´ ë²”ìœ„ë¥¼ 0~4.5 ì •ë„ë¡œ ì¡ì•„ì£¼ë©´ ê¹”ë”í•©ë‹ˆë‹¤.
                ax.set_xlim(min(0, p0 - 0.5), max(4, p100 + 1.5))
                
                # # 4. í†µê³„ ì •ë³´ í…ìŠ¤íŠ¸ ë°•ìŠ¤ ì¶”ê°€
                # stats_text = (f"Count: {count} ({percentage:.1f}%)\n"
                #               f"Outlier Count: {outlier}"
                #               f"Z-Score Mean/Std: {z_avg:.3f}/{z_std:.3f}\n")
                
                # # ê·¸ë˜í”„ ë‚´ ìš°ì¸¡ ìƒë‹¨ì— ë°•ìŠ¤ ë°°ì¹˜
                # ax.text(0.05, 0.5, stats_text, transform=ax.transAxes, fontsize=10,
                #         verticalalignment='top', horizontalalignment='right',
                #         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
                
                # ì„¤ì •
                ax.set_title(f"Exeriment: {experiment_code} | GT: {gt_label} | PRED: {pred_label}\nCount: {count} ({percentage:.1f}%) | Outlier Count: {outlier}", fontsize=14, fontweight='bold')
                ax.set_xlabel("Z-Score (x-axis)")
                ax.set_ylabel("Ratio Score (y-axis)")
                ax.grid(True, linestyle=':', alpha=0.7)
                ax.set_ylim(-0.1, 1.1) # Ratio ScoreëŠ” 0~1 ì‚¬ì´ì´ë¯€ë¡œ

            plt.tight_layout()
            
            # 5. ì €ì¥
            png_file_path = os.path.join(log_save_dir, f"{gt_label}_inference_results.png")
            plt.savefig(png_file_path)
            plt.close(fig)
            print(f"ğŸ“Š ì‹œê°í™” ì™„ë£Œ: {png_file_path}")

        # # --- ì‹œê°í™” ì„¹ì…˜ ---
        # sns.set_theme(style="whitegrid")
        # plt.rcParams['font.family'] = 'NanumGothic' 
        # plt.rcParams['axes.unicode_minus'] = False

        # gt_labels = list(final_results.keys())
        # n_labels = len(gt_labels)
        # n_cols = 2
        # n_rows = math.ceil(n_labels / n_cols)

        # fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 5 * n_rows))
        # if n_labels == 1: axes = [axes] # ë¼ë²¨ì´ 1ê°œì¼ ê²½ìš° ëŒ€ë¹„
        # else: axes = axes.flatten()

        # for idx, gt_label in enumerate(gt_labels):
        #     ax = axes[idx]
        #     df = pd.DataFrame(final_results[gt_label])
            
        #     # Xì¶• ì •ë ¬ ë° ê°œìˆ˜(n=) í¬í•¨ ë¼ë²¨ ìƒì„±
        #     sorted_labels = sorted(df['label'].unique())
        #     label_counts = df['label'].value_counts()
        #     xtick_with_counts = [f"{l}\n(n={label_counts[l]})" for l in sorted_labels]
            
        #     # í†µê³„ ê³„ì‚°
        #     total_count = len(df)
        #     match_count = len(df[df['label'] == gt_label])
        #     mismatch_count = total_count - match_count
        #     match_ratio = (match_count / total_count * 100) if total_count > 0 else 0
            
        #     # Blending Plot (Strip + Box)
        #     sns.stripplot(data=df, x='label', y='score', order=sorted_labels,
        #                   ax=ax, jitter=0.2, size=4, alpha=0.5, palette="magma")
        #     sns.boxplot(data=df, x='label', y='score', order=sorted_labels,
        #                 ax=ax, whis=np.inf, color="0.9", width=0.4, boxprops=dict(alpha=0.3))

        #     # ì œëª© ë° ì¶• ì„¤ì •
        #     title_str = (f"GT: {gt_label}\n"
        #                 f"MATCH: {match_count} ({match_ratio:.1f}%) | "
        #                 f"MISMATCH: {mismatch_count}")
            
        #     ax.set_title(title_str, fontsize=14, fontweight='bold', pad=15)
        #     ax.set_xticks(range(len(sorted_labels)))
        #     ax.set_xticklabels(xtick_with_counts, rotation=30, ha='right')
        #     ax.set_ylim(-0.05, 1.05)
        #     ax.set_xlabel("Predicted Labels (Count)", fontsize=10)
        #     ax.set_ylabel("Confidence Score", fontsize=10)

        # # ë¹ˆ ì„œë¸Œí”Œë¡¯ ì œê±°
        # for j in range(idx + 1, len(axes)):
        #     fig.delaxes(axes[j])

        # plt.tight_layout()
        # png_file_path = os.path.join(log_save_dir, "total_inference_results.png")
        # plt.savefig(png_file_path, dpi=150)
        # logger.info(f"Saved Plot to {png_file_path}")

        # 5-4. CSV íŒŒì¼ ì¶”ì¶œ
        csv_file_name = f"{experiment_code}_process_4_1_inference_sentences.csv"
        csv_file_path = os.path.join(log_save_dir, csv_file_name)
        
        all_data_for_csv = []
        for gt_label, records in final_results.items():
            for record in records:
                row = record.copy()
                row['gt_label'] = gt_label
                all_data_for_csv.append(row)

        df_final = pd.DataFrame(all_data_for_csv)
        if not df_final.empty:
            cols = ['gt_label'] + [c for c in df_final.columns if c != 'gt_label']
            df_final = df_final[cols]
            df_final.to_csv(csv_file_path, index=False, encoding='utf-8-sig')
            logger.info(f"Saved CSV log to {csv_file_path}")

    logger.info("[Process 4] Completed.")
    return context