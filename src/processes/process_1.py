# src/processes/process_1.py

import torch
import os
import logging
from datetime import datetime

# 1. Modules: ëª¨ë¸ í•™ìŠµ(Trainer)ê³¼ ì—”í‹°í‹° ê¸°ë°˜ í‰ê°€(Evaluator) ëª¨ë“ˆ
from src.modules.ner_trainer import Trainer
from src.modules.ner_evaluator import Evaluator

# 2. Database: DB ì„¸ì…˜ ê´€ë¦¬ ë° ê²°ê³¼ ì ìž¬ CRUD
from src.database.connection import db_manager
from src.database import crud

# 3. Utils: ì‹œê°í™” ë„êµ¬ ë° ê³µí†µ ìœ í‹¸ë¦¬í‹°
from src.utils.visualizer import (
    plot_loss_graph, 
    plot_confusion_matrix_trends, 
    plot_label_relation_matrix,
    plot_label_accuracy_histograms
)
from src.utils.common import ensure_dir, save_logs_to_csv

def run_process_1(config: dict, context: dict):
    """
    [Process 1] ëª¨ë¸ í•™ìŠµ ë° ì—”í‹°í‹° ë ˆë²¨ ë‹¤ê°ë„ ê²€ì¦ í”„ë¡œì„¸ìŠ¤
    
    ì£¼ìš” ì—…ë°ì´íŠ¸ ì‚¬í•­:
    - Evaluatorì—ì„œ ì‚°ì¶œëœ 'ì—”í‹°í‹° ê¸°ë°˜ Precision/Recall/F1' ê¸°ë¡
    - ì˜ë¯¸ ë‹¨ìœ„ ë¼ë²¨(Pure Label) ê¸°ì¤€ì˜ Confusion Matrix ì¶”ì´ ìˆ˜ì§‘
    - ì—”í‹°í‹° ì •í™•ë„ ê°€ì¤‘ì¹˜ ì ìˆ˜($0.5 + 0.5 \times Ratio$) ë¶„í¬ ìˆ˜ì§‘
    """
    
    # ==============================================================================
    # [Step 1] ê°ì²´ ë° ì„¤ì • ì¤€ë¹„
    # ==============================================================================
    experiment_code = context['experiment_code']
    device = context['device']
    model = context['model']
    optimizer = context['optimizer']
    scheduler = context['scheduler']
    train_loader = context['train_loader']
    valid_loader = context['valid_loader']
    preprocessor = context['preprocessor'] 

    train_conf = config['train']
    path_conf = config['path']

    logger = logging.getLogger(experiment_code)
    logger.info(f"ðŸš€ [Process 1] Start Training Loop for {experiment_code}")

    # ==============================================================================
    # [Step 2] ì‹¤í–‰ ëª¨ë“ˆ ì´ˆê¸°í™”
    # ==============================================================================
    trainer = Trainer(model, optimizer, scheduler, device)
    
    # EvaluatorëŠ” ë‚´ë¶€ì ìœ¼ë¡œ BIOë¥¼ ë¶„ë¦¬í•˜ì—¬ ì˜ë¯¸ ë‹¨ìœ„(Entity) ë¶„ì„ì„ ìˆ˜í–‰í•¨
    evaluator = Evaluator(
        model, 
        device, 
        preprocessor.tokenizer, 
        preprocessor.ner_id2label
    )

    # ==============================================================================
    # [Step 3] ì—í¬í¬ ë°˜ë³µ (Training & Evaluation Loop)
    # ==============================================================================
    best_f1 = 0.0
    min_valid_loss = float('inf')
    best_f1_epoch = -1
    min_loss_epoch = -1


    train_losses, valid_losses = [], []
    
    # ì‹œê°í™” ížˆìŠ¤í† ë¦¬ (Trend ë¶„ì„ìš©)
    cm_history = []         # ì—í¬í¬ë³„ ì—”í‹°í‹° ê¸°ë°˜ Confusion Matrix ë°ì´í„°
    accuracy_history = []   # ì—í¬í¬ë³„ ì—”í‹°í‹° ì •í™•ë„ ì ìˆ˜($0.0 \sim 1.0$) ë¶„í¬

    # ì €ìž¥ ê²½ë¡œ ìƒì„±
    ckpt_save_dir = os.path.join(path_conf['checkpoint_dir'], experiment_code)
    log_save_dir = os.path.join(path_conf['log_dir'], experiment_code) 
    ensure_dir(ckpt_save_dir)
    ensure_dir(log_save_dir)

    with db_manager.get_db() as session:
        for epoch in range(1, train_conf['epochs'] + 1):
            logger.info(f"=== Epoch {epoch}/{train_conf['epochs']} ===")
            
            # -----------------------------------------------------------
            # 3-1. í•™ìŠµ (Train Phase)
            # -----------------------------------------------------------
            train_result = trainer.train_epoch(train_loader, epoch)
            train_losses.append(train_result['loss'])
            
            # -----------------------------------------------------------
            # 3-2. ê²€ì¦ (Validation Phase)
            # -----------------------------------------------------------
            # v_metrics['f1']ì€ ì´ì œ í† í° ë‹¨ìœ„ê°€ ì•„ë‹Œ 'ì—”í‹°í‹° ë‹¨ìœ„'ì˜ F1ìž„
            valid_result = evaluator.evaluate(valid_loader, mode="valid")
            v_metrics = valid_result['metrics']
            valid_losses.append(v_metrics['loss'])

            # ì‹œê°í™” ížˆìŠ¤í† ë¦¬ ëˆ„ì 
            if 'confusion_matrix' in v_metrics:
                # v_metrics['confusion_matrix'] êµ¬ì¡°: {"labels": [...], "values": [[...]]}
                cm_history.append(v_metrics['confusion_matrix'])
            
            if 'label_accuracy_distribution' in v_metrics:
                accuracy_history.append(v_metrics['label_accuracy_distribution'])
            
            # -----------------------------------------------------------
            # 3-3. ê²°ê³¼ í†µí•© ë° DB ì €ìž¥ (Epoch ë‹¨ìœ„ ìš”ì•½)
            # -----------------------------------------------------------
            epoch_summary = {
                "train_loss": train_result['loss'],
                "train_time": train_result['duration'],
                "valid_loss": v_metrics['loss'],
                "valid_f1": v_metrics['f1'],           # Entity-level F1
                "valid_precision": v_metrics['precision'],
                "valid_recall": v_metrics['recall'],
                "entity_accuracy_avg": {
                    label: (sum(scores)/len(scores) if scores else 0) 
                    for label, scores in v_metrics.get('label_accuracy_distribution', {}).items()
                },
                "confusion_matrix": v_metrics.get('confusion_matrix') # ì˜ë¯¸ ë‹¨ìœ„ CM
            }
            
            crud.create_process_result(session, {
                "experiment_code": experiment_code,
                "process_code": "process_1",
                "process_epoch": epoch,
                "process_start_time": train_result['start_time'], 
                "process_end_time": valid_result['end_time'], 
                "process_duration": train_result['duration'] + valid_result['duration'],
                "process_results": epoch_summary 
            })
            
            logger.info(f"Epoch {epoch} Result Saved. (Train Loss: {train_result['loss']:.4f} | Valid Loss: {v_metrics['loss']:.4f} | Valid F1: {v_metrics['f1']:.4f})")

            # -----------------------------------------------------------
            # 3-3-2. ë¬¸ìž¥ ë‹¨ìœ„ ì¶”ë¡  ê²°ê³¼ ì €ìž¥ (DB + CSV) [UPDATED]
            # -----------------------------------------------------------
            # FK ì •ë³´ ì£¼ìž…
            valid_logs = valid_result['logs']
            for log in valid_logs:
                log.update({"experiment_code": experiment_code, "process_code": "process_1", "process_epoch": epoch})
            
            # (1) DB Bulk Insert
            crud.bulk_insert_inference_sentences(session, valid_logs)
            logger.info(f"Saved {len(valid_logs)} inference logs to DB.")

            # (2) [NEW] CSV íŒŒì¼ ì¶”ì¶œ ë° ì €ìž¥( ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í˜¸ì¶œ (JSON í•„ë“œëŠ” ë¬¸ìžì—´ë¡œ ë³€í™˜ë˜ì–´ ì €ìž¥ë¨) )
            save_logs_to_csv(valid_logs, os.path.join(log_save_dir, f"{experiment_code}_process_1_{epoch}_inference_sentences.csv"))

            # -----------------------------------------------------------
            # 3-4. ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ (Model Checkpoint)
            # -----------------------------------------------------------
            save_path = os.path.join(ckpt_save_dir, f"{experiment_code}_epoch_{epoch}.pt")
            torch.save(model.state_dict(), save_path)
            
            if v_metrics['f1'] > best_f1:
                best_f1 = v_metrics['f1']
                best_f1_epoch = epoch
                logger.info(f"âœ¨ Current Best F1: {best_f1:.4f} (Epoch {epoch})")

            if v_metrics['loss'] < min_valid_loss:
                min_valid_loss = v_metrics['loss']
                min_loss_epoch = epoch
                logger.info(f"ðŸ“‰ Current Min Loss: {min_valid_loss:.4f} (Epoch {epoch})")

        # ==============================================================================
        # [Step 4] ì‹¤í—˜ ë§ˆìŠ¤í„° ì •ë³´ ê°±ì‹  (ìµœì¢… ì„±ëŠ¥ ê¸°ë¡)
        # ==============================================================================
        crud.update_experiment(session, experiment_code, {
            "experiment_config": {
                "best_f1": best_f1,
                "best_epoch": min_loss_epoch,
                "best_model_path": os.path.join(ckpt_save_dir, f"{experiment_code}_epoch_{min_loss_epoch}.pt")
            }
        })

        context['best_epoch'] = min_loss_epoch
        
    # ==============================================================================
    # [Step 5] ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„± (í•™ìŠµ ì¢…ë£Œ í›„ ì¼ê´„ ìƒì„±)
    # ==============================================================================
    logger.info("ðŸ“Š ì‹œê°í™” ë¦¬í¬íŠ¸ ìƒì„± ì‹œìž‘ (Best Epoch ê¸°ì¤€ ìƒì„¸ ë¶„ì„ ë° ì „ì²´ Trend)")

    # 1. Train/Valid Loss ê³¡ì„  (ì „ì²´ Epoch)
    plot_loss_graph(train_losses, valid_losses, log_save_dir, experiment_code)

    # 2. ì—í¬í¬ë³„ Confusion Matrix Trend ë¶„ì„
    # ê° GT ë¼ë²¨ë³„ë¡œ ì—í¬í¬ê°€ ì§„í–‰ë¨ì— ë”°ë¼ ì–´ë–¤ Pred ë¼ë²¨ë“¤ë¡œ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ ì¶”ì´ë¥¼ ê·¸ë¦¼
    # (ì˜ˆ: GT 'PER'ê°€ Epoch 1ì—ëŠ” 'O'ë¡œ ë§Žì´ ê°€ë‹¤ê°€, ì ì°¨ 'PER'ë¡œ ìˆ˜ë ´í•˜ëŠ” ê³¼ì • ì‹œê°í™”)
    if cm_history:
        plot_confusion_matrix_trends(
            cm_history, 
            log_save_dir, 
            experiment_code
        )

    # 3. ìµœì  Epoch (Min Valid Loss) ê¸°ì¤€ ìƒì„¸ ì‹œê°í™”
    best_idx = min_loss_epoch-1

    if 0 <= best_idx < len(cm_history):
        logger.info(f"âœ¨ ìµœì  Epoch({min_loss_epoch})ì˜ ìƒì„¸ ë¶„ì„ ê·¸ëž˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # (A) ì˜ë¯¸ ë‹¨ìœ„ ë¼ë²¨ ê´€ê³„ ížˆíŠ¸ë§µ (Confusion Matrix)
        # ìµœì  ì‹œì ì— ì–´ë–¤ ë¼ë²¨ë¼ë¦¬ ê°€ìž¥ ë§Žì´ í˜¼ë™ë˜ëŠ”ì§€ í™•ì¸
        plot_label_relation_matrix(
            cm_history[best_idx], 
            log_save_dir, 
            f"{experiment_code}_best_epoch_{min_loss_epoch}"
        )

        # (B) ë¼ë²¨ë³„ ì—”í‹°í‹° ì •í™•ë„ ë¶„í¬ ížˆìŠ¤í† ê·¸ëž¨
        # ìµœì  ì‹œì ì— ëª¨ë¸ì´ ê°œì²´ëª…ì˜ ì‹œìž‘(B)ê³¼ ë²”ìœ„(I)ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•ížˆ ì§šì—ˆëŠ”ì§€ ë¶„í¬ í™•ì¸
        plot_label_accuracy_histograms(
            accuracy_history[best_idx], 
            log_save_dir, 
            f"{experiment_code}_best_epoch_{min_loss_epoch}"
        )
    
    logger.info(f"âœ… ì‹œê°í™” ë¦¬í¬íŠ¸ ì €ìž¥ ì™„ë£Œ: {log_save_dir}")
    
    return context