# src/processes/process_1.py

import torch
import os
import logging
from datetime import datetime

# 1. Modules: í•™ìŠµê³¼ ê²€ì¦ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë“ˆ
from src.modules.ner_trainer import Trainer
from src.modules.ner_evaluator import Evaluator

# 2. Database: DB ì—°ê²° ë° CRUD ìœ í‹¸ë¦¬í‹°
from src.database.connection import db_manager
from src.database import crud

# 3. Utils: ì‹œê°í™” ë° íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë ¨ ë„êµ¬
from src.utils.visualizer import plot_loss_graph, plot_confusion_matrix_trends 
from src.utils.common import ensure_dir, save_logs_to_csv

def run_process_1(config: dict, context: dict):
    """
    [Process 1] ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ë£¨í”„ (Execution Phase)
    
    Process 0ì—ì„œ ì¤€ë¹„ëœ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ë°›ì•„ ì‹¤ì œ í•™ìŠµ(Train)ê³¼ ê²€ì¦(Valid)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ë§¤ Epochë§ˆë‹¤ ê²°ê³¼ ì§€í‘œë¥¼ DBì— ì €ìž¥í•˜ê³ , ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ ì¶”ë¡  ê²°ê³¼ë¥¼ íŒŒì¼(pt, csv)ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.

    Args:
        config (dict): ì„¤ì • íŒŒì¼ ë‚´ìš© (experiment_config.yaml)
        context (dict): Process 0ì—ì„œ ìƒì„±ëœ ê°ì²´ë“¤ (ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ë°ì´í„°ë¡œë” ë“±)

    Returns:
        dict: í•™ìŠµëœ ëª¨ë¸ì´ í¬í•¨ëœ ê°±ì‹ ëœ Context
    """
    
    # ==============================================================================
    # [Step 1] Context Unpacking & Setup (ì¤€ë¹„ ë‹¨ê³„)
    # ==============================================================================
    experiment_code = context['experiment_code']
    device = context['device']
    model = context['model']
    optimizer = context['optimizer']
    scheduler = context['scheduler']
    train_loader = context['train_loader']
    valid_loader = context['valid_loader']
    preprocessor = context['preprocessor'] 

    train_conf = config['train']
    path_conf = config['path']

    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger(experiment_code)
    logger.info(f"ðŸš€ [Process 1] Start Training Loop for {experiment_code}")

    # ==============================================================================
    # [Step 2] Worker ëª¨ë“ˆ ì´ˆê¸°í™”
    # ==============================================================================
    trainer = Trainer(model, optimizer, scheduler, device)
    
    evaluator = Evaluator(
        model, 
        device, 
        preprocessor.tokenizer, 
        preprocessor.ner_id2label
    )

    # ==============================================================================
    # [Step 3] í•™ìŠµ ë£¨í”„ (Training Loop)
    # ==============================================================================
    best_f1 = 0.0
    min_valid_loss = float('inf')
    best_f1_epoch = -1
    min_loss_epoch = -1

    train_losses = []
    valid_losses = []

    cm_history = [] # Graphë¥¼ ìœ„í•œ confusion_matrix history
    
    # ì €ìž¥ ê²½ë¡œ ì¤€ë¹„
    ckpt_save_dir = os.path.join(path_conf['checkpoint_dir'], experiment_code)
    log_save_dir = os.path.join(path_conf['log_dir'], experiment_code) 
    ensure_dir(ckpt_save_dir)
    ensure_dir(log_save_dir)

    # DB ì„¸ì…˜ ì‹œìž‘
    with db_manager.get_db() as session:
        for epoch in range(1, train_conf['epochs'] + 1):
            logger.info(f"=== Epoch {epoch}/{train_conf['epochs']} ===")
            
            # -----------------------------------------------------------
            # 3-1. í•™ìŠµ (Train Phase)
            # -----------------------------------------------------------
            train_result = trainer.train_epoch(train_loader, epoch)
            train_losses.append(train_result['loss'])
            
            # -----------------------------------------------------------
            # 3-2. ê²€ì¦ (Validation Phase)
            # -----------------------------------------------------------
            valid_result = evaluator.evaluate(valid_loader, mode="valid")
            
            valid_metrics = valid_result['metrics']
            valid_logs = valid_result['logs']
            valid_losses.append(valid_metrics['loss'])

            if 'confusion_matrix' in valid_metrics:
                cm_history.append(valid_metrics['confusion_matrix'])
            
            # -----------------------------------------------------------
            # 3-3. ê²°ê³¼ í†µí•© ë° DB ì €ìž¥ (Epoch ë‹¨ìœ„ ìš”ì•½)
            # -----------------------------------------------------------
            
            # (1) ëª¨ë“  ì§€í‘œ í†µí•© (JSONBìš©)
            epoch_all_metrics = {
                "train_loss": train_result['loss'],
                "train_time": train_result['duration'],
                "valid_loss": valid_metrics['loss'],
                "valid_precision": valid_metrics['precision'],
                "valid_recall": valid_metrics['recall'],
                "valid_f1": valid_metrics['f1'],
                "valid_time": valid_result['duration'],
                "confusion_matrix": valid_metrics['confusion_matrix']
            }
            
            # (2) DB ì €ìž¥ (ExperimentProcessResult)
            crud.create_process_result(session, {
                "experiment_code": experiment_code,
                "process_code": "process_1",
                "process_epoch": epoch,
                "process_start_time": train_result['start_time'], 
                "process_end_time": valid_result['end_time'], 
                "process_duration": train_result['duration'] + valid_result['duration'],
                "process_results": epoch_all_metrics 
            })
            
            logger.info(f"Epoch {epoch} Result Saved. (Train Loss: {train_result['loss']:.4f} | Valid F1: {valid_metrics['f1']:.4f})")

            # -----------------------------------------------------------
            # 3-3-2. ë¬¸ìž¥ ë‹¨ìœ„ ì¶”ë¡  ê²°ê³¼ ì €ìž¥ (DB + CSV) [UPDATED]
            # -----------------------------------------------------------
            # FK ì •ë³´ ì£¼ìž…
            for log in valid_logs:
                log['experiment_code'] = experiment_code
                log['process_code'] = "process_1"
                log['process_epoch'] = epoch
            
            # (1) DB Bulk Insert
            crud.bulk_insert_inference_sentences(session, valid_logs)
            logger.info(f"Saved {len(valid_logs)} inference logs to DB.")

            # (2) [NEW] CSV íŒŒì¼ ì¶”ì¶œ ë° ì €ìž¥
            csv_file_name = f"{experiment_code}_process_1_{epoch}_inference_sentences.csv"
            csv_file_path = os.path.join(log_save_dir, csv_file_name)
            
            # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í˜¸ì¶œ (JSON í•„ë“œëŠ” ë¬¸ìžì—´ë¡œ ë³€í™˜ë˜ì–´ ì €ìž¥ë¨)
            save_logs_to_csv(valid_logs, csv_file_path)

            # -----------------------------------------------------------
            # 3-4. ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ (Model Checkpoint)
            # -----------------------------------------------------------
            # íŒŒì¼ëª… í†µì¼: {code}_epoch_{epoch}.pt
            save_name = f"{experiment_code}_epoch_{epoch}.pt"
            save_path = os.path.join(ckpt_save_dir, save_name)
            torch.save(model.state_dict(), save_path)
            
            # Best ê¸°ë¡ ê°±ì‹ 
            if valid_metrics['f1'] > best_f1:
                best_f1 = valid_metrics['f1']
                best_f1_epoch = epoch
                logger.info(f"âœ¨ Current Best F1: {best_f1:.4f} (Epoch {epoch})")
            
            if valid_metrics['loss'] < min_valid_loss:
                min_valid_loss = valid_metrics['loss']
                min_loss_epoch = epoch
                logger.info(f"ðŸ“‰ Current Min Loss: {min_valid_loss:.4f} (Epoch {epoch})")

        # ==============================================================================
        # [Step 4] ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (DB Update)
        # ==============================================================================
        exp_obj = crud.get_experiment(session, experiment_code)
        if exp_obj:
            current_config = exp_obj.experiment_config or {}
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê²½ë¡œ êµ¬ì„± (íŒŒì¼ëª… í˜•ì‹ ì¼ì¹˜ì‹œí‚´)
            best_f1_path = os.path.join(ckpt_save_dir, f"{experiment_code}_epoch_{best_f1_epoch}.pt")
            min_loss_path = os.path.join(ckpt_save_dir, f"{experiment_code}_epoch_{min_loss_epoch}.pt")
            
            current_config['best_model_f1_path'] = best_f1_path
            current_config['best_model_loss_path'] = min_loss_path
            current_config['best_f1_score'] = best_f1
            current_config['min_valid_loss'] = min_valid_loss
            
            crud.update_experiment(session, experiment_code, {
                "experiment_config": current_config,
            })
            logger.info(f"âœ… Experiment Meta Updated. (Best F1 Epoch: {best_f1_epoch})")

    # ==============================================================================
    # [Step 5] ë§ˆë¬´ë¦¬ ë° ì‹œê°í™” (Finalize)
    # ==============================================================================
    # í•™ìŠµ ì¢…ë£Œ í›„ Loss ê·¸ëž˜í”„ ì €ìž¥ (log_save_dir ì‚¬ìš©)
    plot_loss_graph(
        train_losses, 
        valid_losses, 
        log_save_dir, 
        experiment_code
    )

    # 2. [NEW] Label Distribution Graph (Confusion Matrix Trend)
    # preprocessor.ner_id2labelì„ ë„˜ê²¨ì¤˜ì„œ IDë¥¼ ë¼ë²¨ëª…(B-PER ë“±)ìœ¼ë¡œ ë³€í™˜
    plot_confusion_matrix_trends(
        cm_history, 
        preprocessor.ner_id2label, 
        log_save_dir, 
        experiment_code
    )
    
    logger.info("[Process 1] Process Completed Successfully.")
    
    # í•™ìŠµëœ ëª¨ë¸ ê°ì²´ë¥¼ í¬í•¨í•˜ì—¬ Context ë°˜í™˜
    return context