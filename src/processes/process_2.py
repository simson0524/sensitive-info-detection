# src/processes/process_2.py

import logging
import os
from datetime import datetime
from tqdm import tqdm

# Modules: ì‚¬ì „ ë§¤ì¹­ ë° ê²°ê³¼ ì§‘ê³„ë¥¼ ìœ„í•œ í•µì‹¬ ëª¨ë“ˆ
from src.modules.dictionary_matcher import DictionaryMatcher
from src.modules.result_aggregator import ResultAggregator

# Database: DB ì—°ê²° ë° CRUD ìœ í‹¸ë¦¬í‹°
from src.database.connection import db_manager
from src.database import crud

# Utils: íŒŒì¼ ì €ì¥ ê´€ë ¨ ìœ í‹¸ë¦¬í‹°
from src.utils.common import ensure_dir, save_logs_to_csv

def run_process_2(config: dict, context: dict):
    """
    [Process 2] ì‚¬ì „(Dictionary) ë§¤ì¹­ ê²€ì¦ í”„ë¡œì„¸ìŠ¤
    
    ê¸° êµ¬ì¶•ëœ ì‚¬ì „ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ PIIë¥¼ íƒì§€í•˜ê³  ì„±ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. DBì—ì„œ í•´ë‹¹ ë„ë©”ì¸ì˜ ì‚¬ì „ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ (Dictionary Matcher)
    2. ê²€ì¦ ë°ì´í„°ì…‹ì„ ìˆœíšŒí•˜ë©° ì‚¬ì „ íƒìƒ‰ ìˆ˜í–‰ (ë¬¸ì¥ ì „ì²´ ê²€ìƒ‰)
    3. [Train Mode] ì •ë‹µ(GT)ê³¼ ë¹„êµí•˜ì—¬ ì •íƒ/ì˜¤íƒ/ë¯¸íƒ ê²€ì¦
       -> ì˜¤íƒ(Wrong) ë°œìƒ ì‹œ, í•´ë‹¹ ë‹¨ì–´ë¥¼ ì‚¬ì „ì—ì„œ ì¦‰ì‹œ ë¬´íš¨í™”(Self-Cleaning)
    4. [Test Mode] ë¬¸ì¥ ë‚´ í¬í•¨ëœ ì‚¬ì „ ë‹¨ì–´ ë‹¨ìˆœ íƒì§€ ë° ì €ì¥
    5. ê²°ê³¼ ë¡œê·¸ DB ì €ì¥ ë° CSV íŒŒì¼ ì¶”ì¶œ

    Args:
        config (dict): ì„¤ì • ì •ë³´
        context (dict): ê³µìœ  ê°ì²´ (DataLoader, Preprocessor ë“±)
    """
    
    # ==============================================================================
    # [Step 1] ì„¤ì • ë¡œë“œ ë° ë¡œê±° ì´ˆê¸°í™”
    # ==============================================================================
    exp_conf = config['experiment']
    dict_conf = config['dictionary_init']
    train_conf = config['train']
    path_conf = config['path'] # [NEW] CSV ì €ì¥ìš© ê²½ë¡œ
    
    experiment_code = exp_conf['experiment_code']
    data_category = exp_conf.get('data_category', 'personal_data') # 'personal_data' or 'confidential_data'
    run_mode = exp_conf.get('run_mode', 'train')
    
    # ë¡œê±° ê°€ì ¸ì˜¤ê¸°
    logger = logging.getLogger(experiment_code)
    logger.info(f"ğŸš€ [Process 2] Start Dictionary Matching Verification (Mode: {run_mode})")

    # ==============================================================================
    # [Step 2] ë°ì´í„° ë° ë§¤í•‘ ì •ë³´ ì¤€ë¹„
    # ==============================================================================
    valid_loader = context['valid_loader']
    
    # GT íŒŒì‹±ì„ ìœ„í•œ ë„êµ¬ë“¤ (Train ëª¨ë“œì—ì„œ í•„ìˆ˜)
    preprocessor = context['preprocessor']
    tokenizer = preprocessor.tokenizer
    ner_id2label = preprocessor.ner_id2label

    # ê²€ì¦ ëŒ€ìƒ ë¼ë²¨ ì„¤ì • (ì˜ˆ: 'ê°œì¸ì •ë³´')
    target_label_name = "ê°œì¸ì •ë³´" if data_category == "personal_data" else "ê¸°ë°€ì •ë³´"
    
    # í†µê³„ìš© ë¼ë²¨ ID (ì—†ìœ¼ë©´ ê±´ë„ˆëœ€)
    pred_label_id = train_conf['label_map'].get(target_label_name)
    if pred_label_id is None:
        logger.warning(f"âš ï¸ Target label '{target_label_name}' not found. Skipping Process 2.")
        return context

    # ==============================================================================
    # [Step 3] ì‚¬ì „ ë§¤ì²˜(Matcher) ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë“œ
    # ==============================================================================
    with db_manager.get_db() as session:
        matcher = DictionaryMatcher(session)
        
        # ì„¤ì •ëœ ë„ë©”ì¸ IDë“¤ì— í•´ë‹¹í•˜ëŠ” ì‚¬ì „ì„ DBì—ì„œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ì— ìºì‹±
        # (Insertion > Deletion ì¸ ìœ íš¨ ë‹¨ì–´ë§Œ ë¡œë“œë¨)
        matcher.load_dictionaries(dict_conf['domain_ids'], data_category)
        
        # ë¡œë“œëœ ì‚¬ì „ì˜ í¬ê¸° ë“± í†µê³„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        dict_stats = matcher.get_stats()
        logger.info(f"ğŸ“š Dictionary Stats: {dict_stats}")

    # ==============================================================================
    # [Step 4] ë§¤ì¹­ ë° ê²€ì¦ ë£¨í”„ (Validation Loop)
    # ==============================================================================
    aggregator = ResultAggregator() # ê²°ê³¼(ì •/ì˜¤/ë¯¸íƒ)ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê°ì²´
    start_time = datetime.now()
    process_epoch = 1 # Rule-base ê²€ì¦ì€ 1íšŒì„± í”„ë¡œì„¸ìŠ¤ì´ë¯€ë¡œ Epoch 1ë¡œ ê³ ì •

    logger.info("Starting matching loop...")
    
    # ë¡œê·¸ ì €ì¥ ê²½ë¡œ ìƒì„± (CSV ì €ì¥ìš©)
    log_save_dir = os.path.join(path_conf['log_dir'], experiment_code)
    ensure_dir(log_save_dir)
    
    # [ì¤‘ìš”] ì˜¤íƒ ì‹œ ì‚¬ì „ ì—…ë°ì´íŠ¸(Deletion Count ì¦ê°€)ë¥¼ ìœ„í•´ ì„¸ì…˜ì„ ë£¨í”„ ë°–ì—ì„œ ì—½ë‹ˆë‹¤.
    with db_manager.get_db() as session:
        
        for batch in tqdm(valid_loader, desc="Dictionary Matching"):
            batch_size = len(batch['sentence'])
            
            # Tensor -> List ë³€í™˜ (CPUë¡œ ì´ë™)
            input_ids_batch = batch['input_ids'].cpu().tolist()
            labels_batch = batch['labels'].cpu().tolist()

            for i in range(batch_size):
                # -----------------------------------------------------------
                # 4-1. ë©”íƒ€ ë°ì´í„° ì¶”ì¶œ
                # -----------------------------------------------------------
                sentence_id = batch['sentence_id'][i]
                original_sentence = batch['sentence'][i]
                domain_id = batch['domain_id'][i]
                file_name = batch['file_name'][i]
                
                # Tensor -> Item ì•ˆì „ ë³€í™˜
                seq_val = batch['sentence_seq'][i]
                sentence_seq = seq_val.item() if hasattr(seq_val, 'item') else seq_val

                # -----------------------------------------------------------
                # 4-2. ì‚¬ì „ íƒìƒ‰ (ê³µí†µ ë¡œì§)
                # -----------------------------------------------------------
                # ë¬¸ì¥ ì „ì²´ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì‚¬ì „ì— ìˆëŠ” ë‹¨ì–´ë“¤ì„ ëª¨ë‘ ì°¾ìŠµë‹ˆë‹¤.
                # (match_sentenceëŠ” Listë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ Setìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¤‘ë³µ ì œê±°)
                dict_matches = set(matcher.match_sentence(original_sentence, domain_id))

                # -----------------------------------------------------------
                # [Case A] Train Mode (BIO íƒœê·¸ íŒŒì‹± í›„ ë¹„êµ + ì˜¤íƒ ì œê±°)
                # -----------------------------------------------------------
                if run_mode == 'train':
                    # (1) ì •ë‹µ(GT) ì¶”ì¶œ: BIO íƒœê·¸ -> ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                    current_input_ids = input_ids_batch[i]
                    current_tags = labels_batch[i]
                    tokens = tokenizer.convert_ids_to_tokens(current_input_ids)
                    
                    gt_entities = _extract_entities_from_bio(tokens, current_tags, ner_id2label, tokenizer)
                    
                    # í˜„ì¬ íƒ€ê²Ÿ ì¹´í…Œê³ ë¦¬(ì˜ˆ: ê°œì¸ì •ë³´)ì— í•´ë‹¹í•˜ëŠ” GTë§Œ í•„í„°ë§
                    target_gt_words = {
                        word for word, label in gt_entities.items() 
                        if label == target_label_name
                    }

                    # (2) ì •íƒ/ì˜¤íƒ/ë¯¸íƒ ë¶„ë¥˜ (ì§‘í•© ì—°ì‚°)
                    hits = target_gt_words & dict_matches       # êµì§‘í•© (ë‘˜ ë‹¤ ìˆìŒ)
                    mismatches = target_gt_words - dict_matches # ì°¨ì§‘í•© (GTì—” ìˆëŠ”ë° ì‚¬ì „ì—” ì—†ìŒ)
                    wrongs = dict_matches - target_gt_words     # ì°¨ì§‘í•© (ì‚¬ì „ì—” ìˆëŠ”ë° GTì—” ì—†ìŒ)

                    # (3) ë¡œê·¸ ê¸°ë¡ ë° ì‚¬ì „ ì—…ë°ì´íŠ¸
                    
                    # Hit (ì •íƒ)
                    for word in hits:
                        _add_log(aggregator, "hit", sentence_id, file_name, sentence_seq, original_sentence, 
                                 word, domain_id, target_label_name, target_label_name, experiment_code)
                    
                    # Mismatch (ë¯¸íƒ)
                    for word in mismatches:
                        _add_log(aggregator, "mismatch", sentence_id, file_name, sentence_seq, original_sentence, 
                                 word, domain_id, target_label_name, "O", experiment_code)
                                 
                    # Wrong (ì˜¤íƒ) -> ì‚¬ì „ì—ì„œ ë¬´íš¨í™”
                    for word in wrongs:
                        _add_log(aggregator, "wrong", sentence_id, file_name, sentence_seq, original_sentence, 
                                 word, domain_id, "O", target_label_name, experiment_code)
                        
                        # [í•µì‹¬] ì˜¤íƒ ë‹¨ì–´ëŠ” ì¦‰ì‹œ ë¬´íš¨í™” (Deletion Count = Insertion Count)
                        # ë‹¤ìŒë²ˆ ë¡œë“œë¶€í„°ëŠ” (Insertion > Deletion) ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•Šì•„ ì œì™¸ë¨
                        crud.invalidate_dictionary_item(
                            session, word, data_category, domain_id
                        )

                # -----------------------------------------------------------
                # [Case B] Test Mode (ë‹¨ìˆœ íƒì§€ ê²°ê³¼ ì €ì¥)
                # -----------------------------------------------------------
                elif run_mode == 'test':
                    if dict_matches:
                        inference_results = []
                        for word in dict_matches:
                            inference_results.append({
                                "word": word,
                                "label": target_label_name,
                                "match_result": "prediction" # ì •ë‹µì„ ëª¨ë¥´ë¯€ë¡œ predictionìœ¼ë¡œ í‘œê¸°
                            })
                        
                        # JSON êµ¬ì¡° ìƒì„±
                        sentence_inference_result = {
                            "sentence_id": sentence_id,
                            "source_file_name": file_name,
                            "sequence_in_file": sentence_seq,
                            "origin_sentence": original_sentence,
                            "domain_id": domain_id,
                            "inference_results": inference_results,
                            "entity_count": len(inference_results)
                        }
                        
                        log_entry = {
                            "experiment_code": experiment_code,
                            "process_code": "process_2",
                            "process_epoch": process_epoch,
                            "sentence_id": sentence_id,
                            "sentence_inference_result": sentence_inference_result,
                            "confidence_score": 1.0
                        }
                        # Test ëª¨ë“œì—ì„œëŠ” ëª¨ë‘ Hitìœ¼ë¡œ ê°„ì£¼í•˜ê±°ë‚˜ ë³„ë„ ì²˜ë¦¬
                        aggregator.add_result("hit", log_entry, pred_label_id)

        # (ë£¨í”„ ì¢…ë£Œ í›„ ì‹œê°„ ê¸°ë¡)
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # ==============================================================================
        # [Step 5] ìµœì¢… DB ì €ì¥ ë° CSV ì¶”ì¶œ
        # ==============================================================================
        
        # 5-1. ë¬¸ì¥ ë‹¨ìœ„ ìƒì„¸ ë¡œê·¸ ì €ì¥ (Bulk Insert)
        total_logs = 0
        all_logs_for_csv = [] # CSV ì €ì¥ì„ ìœ„í•œ í†µí•© ë¦¬ìŠ¤íŠ¸

        for r_type in ["hit", "wrong", "mismatch"]:
            logs = aggregator.get_logs(r_type)
            if logs:
                # DBì— ëŒ€ëŸ‰ ì‚½ì…
                crud.bulk_insert_inference_sentences(session, logs)
                # CSVìš© ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                all_logs_for_csv.extend(logs) 
                total_logs += len(logs)
        
        logger.info(f"Saved {total_logs} inference logs to DB.")

        # 5-2. [NEW] CSV íŒŒì¼ ì¶”ì¶œ ë° ì €ì¥
        if all_logs_for_csv:
            csv_file_name = f"{experiment_code}_process_2_{process_epoch}_inference_sentences.csv"
            csv_file_path = os.path.join(log_save_dir, csv_file_name)
            
            save_logs_to_csv(all_logs_for_csv, csv_file_path)
            logger.info(f"Saved CSV log to {csv_file_path}")

        # 5-3. í”„ë¡œì„¸ìŠ¤ ìš”ì•½ ì €ì¥
        process_results = {
            "dictionary_stats": dict_stats,
            "metrics": aggregator.get_metrics(),
            "run_mode": run_mode
        }
        
        crud.create_process_result(session, {
            "experiment_code": experiment_code,
            "process_code": "process_2",
            "process_epoch": process_epoch,
            "process_start_time": start_time,
            "process_end_time": end_time,
            "process_duration": duration,
            "process_results": process_results
        })
        
    logger.info("[Process 2] Completed Successfully.")
    return context


# ------------------------------------------------------------------------------
# Helper Functions
# ------------------------------------------------------------------------------

def _extract_entities_from_bio(tokens, tags, id2label, tokenizer):
    """
    BIO íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ {ë‹¨ì–´: ë¼ë²¨} ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    """
    entities = {}
    current_tokens = []
    current_label = None
    
    for token, tag_id in zip(tokens, tags):
        if tag_id == -100: continue
        label_name = id2label.get(tag_id, "O")
        
        if label_name.startswith("B-"):
            if current_tokens:
                word = tokenizer.convert_tokens_to_string(current_tokens)
                entities[word] = current_label
            current_tokens = [token]
            current_label = label_name[2:]
            
        elif label_name.startswith("I-") and current_label == label_name[2:]:
            current_tokens.append(token)
            
        else: # "O"
            if current_tokens:
                word = tokenizer.convert_tokens_to_string(current_tokens)
                entities[word] = current_label
            current_tokens = []
            current_label = None
            
    if current_tokens:
        word = tokenizer.convert_tokens_to_string(current_tokens)
        entities[word] = current_label
        
    return entities

def _add_log(aggregator, match_type, sent_id, fname, seq, origin_sent, word, domain, gt, pred, exp_code, epoch=1, is_hit=None):
    """ë¡œê·¸ ë°ì´í„° ìƒì„± ë° ì§‘ê³„ê¸°ì— ì¶”ê°€"""
    sentence_inference_result = {
        "sentence_id": sent_id,
        "source_file_name": fname,
        "sequence_in_file": seq,
        "origin_sentence": origin_sent,
        "domain_id": domain,
        "inference_results": [{
            "word": word,
            "label": pred,
            "match_result": match_type,
            "ground_truth": gt
        }],
        "entity_count": 1
    }
    
    log_entry = {
        "experiment_code": exp_code,
        "process_code": "process_2",
        "process_epoch": epoch,
        "sentence_id": sent_id,
        "sentence_inference_result": sentence_inference_result,
        "confidence_score": 1.0
    }
    
    # í†µê³„ìš© IDëŠ” ì„ì˜ê°’(0) ì‚¬ìš©
    aggregator.add_result(match_type, log_entry, 0)