# src/processes/process_3.py

import logging
import os
from datetime import datetime
from tqdm import tqdm

# Modules
from src.modules.regex_matcher import RegexMatcher
from src.modules.result_aggregator import ResultAggregator

# Database
from src.database.connection import db_manager
from src.database import crud

# Utils
from src.utils.common import ensure_dir, save_logs_to_csv

def run_process_3(config: dict, context: dict):
    """
    [Process 3] ì •ê·œí‘œí˜„ì‹(Regex) ë§¤ì¹­ ê²€ì¦ í”„ë¡œì„¸ìŠ¤
    
    - ê³µí†µ: RegexMatcherë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ì „ì²´ì—ì„œ PII íƒì§€ ìˆ˜í–‰
    - Train ëª¨ë“œ: BIO íƒœê·¸ë¥¼ íŒŒì‹±í•œ 'ì •ë‹µ ë‹¨ì–´'ì™€ ì •ê·œì‹ íƒì§€ ê²°ê³¼ë¥¼ 1:1 ë¹„êµ (ì •íƒ/ì˜¤íƒ/ë¯¸íƒ)
                  (ë¼ë²¨ Normalization ì ìš©: ê°œì¸ì •ë³´_1 -> ê°œì¸ì •ë³´)
    - Test ëª¨ë“œ: ì •ë‹µ ì—†ì´ ì •ê·œì‹ìœ¼ë¡œ íƒì§€ëœ ê²°ê³¼ë¥¼ ëª¨ë‘ ì €ì¥
    - ê²°ê³¼: DB ì €ì¥ ë° CSV ì¶”ì¶œ
    """
    
    # ==============================================================================
    # [Step 1] ì„¤ì • ë° ë¡œê±° ì´ˆê¸°í™”
    # ==============================================================================
    exp_conf = config['experiment']
    train_conf = config['train']
    path_conf = config['path']
    
    experiment_code = exp_conf['experiment_code']
    data_category = exp_conf.get('data_category', 'personal_data')
    run_mode = exp_conf.get('run_mode', 'train')
    
    logger = logging.getLogger(experiment_code)
    logger.info(f"ğŸš€ [Process 3] Start Regex Matching Verification (Mode: {run_mode})")

    # ==============================================================================
    # [Step 2] ë°ì´í„° ë° ë„êµ¬ ì¤€ë¹„
    # ==============================================================================
    valid_loader = context['valid_loader']
    preprocessor = context['preprocessor']
    tokenizer = preprocessor.tokenizer
    
    # BIO ë¼ë²¨ ë§µ (ID <-> Name) {0: "O", 1: "B-ê°œì¸ì •ë³´_1", ...}
    ner_id2label = preprocessor.ner_id2label 

    # RegexMatcher ì´ˆê¸°í™” (ë‚´ë¶€ì ìœ¼ë¡œ Detectors ë¡œë“œ)
    matcher = RegexMatcher()

    # ==============================================================================
    # [Step 3] ë§¤ì¹­ ë° ê²€ì¦ ë£¨í”„
    # ==============================================================================
    aggregator = ResultAggregator()
    start_time = datetime.now()
    process_epoch = 1

    logger.info("Starting regex detection loop...")
    
    log_save_dir = os.path.join(path_conf['log_dir'], experiment_code)
    ensure_dir(log_save_dir)
    
    for batch in tqdm(valid_loader, desc="Regex Matching"):
        batch_size = len(batch['sentence'])
        
        input_ids_batch = batch['input_ids'].cpu().tolist()
        labels_batch = batch['labels'].cpu().tolist()

        for i in range(batch_size):
            # 3-1. ë©”íƒ€ ë°ì´í„° ì¶”ì¶œ
            sentence_id = batch['sentence_id'][i]
            original_sentence = batch['sentence'][i]
            file_name = batch['file_name'][i]
            domain_id = batch['domain_id'][i]
            sentence_seq = batch['sentence_seq'][i]
            
            seq_val = sentence_seq.item() if hasattr(sentence_seq, 'item') else sentence_seq

            # 3-2. Regex íƒì§€ ìˆ˜í–‰ (ë¬¸ì¥ ì „ì²´ ìŠ¤ìº”)
            regex_results = matcher.detect(original_sentence)
            
            # 3-3. Regex ê²°ê³¼ë¥¼ í”„ë¡œì íŠ¸ ë¼ë²¨ë¡œ ë§¤í•‘ ë° í•„í„°ë§
            pred_spans = {}
            for res in regex_results:
                raw_label = res['label'] # "ì „í™”ë²ˆí˜¸"
                type_info = matcher.DETECTOR_TYPE_MAP.get(raw_label, {})
                
                target_label = None
                if data_category == "personal_data" and type_info.get("category") == "ê°œì¸":
                    target_label = "ê°œì¸ì •ë³´" 
                elif data_category == "confidential_data" and type_info.get("category") == "ê¸°ë°€":
                    target_label = "ê¸°ë°€ì •ë³´"
                
                if target_label:
                    pred_spans[res['match']] = target_label

            # -----------------------------------------------------------
            # [Case A] Train Mode (GTì™€ ë¹„êµí•˜ì—¬ ì •ë°€ ê²€ì¦)
            # -----------------------------------------------------------
            if run_mode == 'train':
                current_input_ids = input_ids_batch[i]
                current_tags = labels_batch[i]
                tokens = tokenizer.convert_ids_to_tokens(current_input_ids)
                
                # GT íŒŒì‹± (e.g., {'í™ê¸¸ë™': 'ê°œì¸ì •ë³´_1'})
                gt_entities = _extract_entities_from_bio(tokens, current_tags, ner_id2label, tokenizer)
                
                # [ìˆ˜ì •] ë¼ë²¨ Normalization (ê°œì¸ì •ë³´_1 -> ê°œì¸ì •ë³´)
                normalized_gt_entities = {
                    word: _normalize_label(label) 
                    for word, label in gt_entities.items()
                }
                
                expected_label_name = "ê°œì¸ì •ë³´" if data_category == "personal_data" else "ê¸°ë°€ì •ë³´"
                
                # íƒ€ê²Ÿ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
                target_gt_spans = {
                    word: label for word, label in normalized_gt_entities.items()
                    if label == expected_label_name
                }

                # ì •íƒ/ì˜¤íƒ/ë¯¸íƒ ë¶„ë¥˜
                pred_words = set(pred_spans.keys())
                gt_words = set(target_gt_spans.keys())

                hits = pred_words & gt_words
                wrongs = pred_words - gt_words
                mismatches = gt_words - pred_words

                # ë¡œê·¸ ê¸°ë¡
                for word in hits:
                    _add_log(aggregator, "hit", sentence_id, file_name, seq_val, 
                             original_sentence, word, domain_id, 
                             expected_label_name, expected_label_name, experiment_code, process_epoch)
                
                for word in wrongs:
                    _add_log(aggregator, "wrong", sentence_id, file_name, seq_val, 
                             original_sentence, word, domain_id, 
                             "O", expected_label_name, experiment_code, process_epoch)

                for word in mismatches:
                    _add_log(aggregator, "mismatch", sentence_id, file_name, seq_val,
                             original_sentence, word, domain_id,
                             expected_label_name, "O", experiment_code, process_epoch)

            # -----------------------------------------------------------
            # [Case B] Test Mode (ë‹¨ìˆœ íƒì§€ ê²°ê³¼ ì €ì¥)
            # -----------------------------------------------------------
            elif run_mode == 'test':
                for word, label in pred_spans.items():
                    _add_log(aggregator, "hit", sentence_id, file_name, seq_val,
                             original_sentence, word, domain_id,
                             "Unknown", label, experiment_code, process_epoch)

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    # ==============================================================================
    # [Step 4] ê²°ê³¼ ì €ì¥ (DB & CSV)
    # ==============================================================================
    with db_manager.get_db() as session:
        total_logs = 0
        all_logs_for_csv = []

        # 4-1. í”„ë¡œì„¸ìŠ¤ ê²°ê³¼ ìš”ì•½ ì €ì¥
        process_results = {
            "metrics": aggregator.get_metrics(),
            "detected_count": total_logs,
            "run_mode": run_mode
        }
        
        crud.create_process_result(session, {
            "experiment_code": experiment_code,
            "process_code": "process_3",
            "process_epoch": process_epoch,
            "process_start_time": start_time,
            "process_end_time": end_time,
            "process_duration": duration,
            "process_results": process_results
        })

        # 4-2. ë¬¸ì¥ ë¡œê·¸ ì €ì¥
        for r_type in ["hit", "wrong", "mismatch"]:
            logs = aggregator.get_logs(r_type)
            if logs:
                crud.bulk_insert_inference_sentences(session, logs)
                all_logs_for_csv.extend(logs)
                total_logs += len(logs)
        
        logger.info(f"Saved {total_logs} inference logs to DB.")

        # [NEW] CSV íŒŒì¼ ì¶”ì¶œ
        if all_logs_for_csv:
            csv_file_name = f"{experiment_code}_process_3_{process_epoch}_inference_sentences.csv"
            csv_file_path = os.path.join(log_save_dir, csv_file_name)
            save_logs_to_csv(all_logs_for_csv, csv_file_path)
            logger.info(f"Saved CSV log to {csv_file_path}")
        
    logger.info("[Process 3] Completed Successfully.")
    return context


# ------------------------------------------------------------------------------
# Helper Functions
# ------------------------------------------------------------------------------

def _normalize_label(label: str) -> str:
    """
    ë¼ë²¨ ì •ê·œí™”: 'ê°œì¸ì •ë³´_1' -> 'ê°œì¸ì •ë³´'
    """
    if "_" in label and label.split("_")[-1].isdigit():
        return label.rsplit("_", 1)[0]
    return label

def _extract_entities_from_bio(tokens, tags, id2label, tokenizer):
    """
    BIO íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ {ë‹¨ì–´: ë¼ë²¨} ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    """
    entities = {}
    current_tokens = []
    current_label = None
    
    for token, tag_id in zip(tokens, tags):
        if tag_id == -100: continue
        label_name = id2label.get(tag_id, "O")
        
        if label_name.startswith("B-"):
            if current_tokens:
                word = tokenizer.convert_tokens_to_string(current_tokens)
                entities[word] = current_label
            current_tokens = [token]
            current_label = label_name[2:]
            
        elif label_name.startswith("I-") and current_label == label_name[2:]:
            current_tokens.append(token)
            
        else:
            if current_tokens:
                word = tokenizer.convert_tokens_to_string(current_tokens)
                entities[word] = current_label
            current_tokens = []
            current_label = None
            
    if current_tokens:
        word = tokenizer.convert_tokens_to_string(current_tokens)
        entities[word] = current_label
        
    return entities

def _add_log(aggregator, match_type, sent_id, fname, seq, origin_sent, word, domain, gt, pred, exp_code, epoch=1):
    """ë¡œê·¸ ë°ì´í„° ìƒì„± ë° ì§‘ê³„ê¸°ì— ì¶”ê°€"""
    sentence_inference_result = {
        "sentence_id": sent_id,
        "source_file_name": fname,
        "sequence_in_file": seq,
        "origin_sentence": origin_sent,
        "domain_id": domain,
        "inference_results": [{
            "word": word,
            "label": pred,
            "match_result": match_type,
            "ground_truth": gt
        }],
        "entity_count": 1
    }
    
    log_entry = {
        "experiment_code": exp_code,
        "process_code": "process_3",
        "process_epoch": epoch,
        "sentence_id": sent_id,
        "sentence_inference_result": sentence_inference_result,
        "confidence_score": 1.0
    }
    
    aggregator.add_result(match_type, log_entry, 0)