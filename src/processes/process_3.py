# src/processes/process_3.py

import logging
from datetime import datetime
from tqdm import tqdm

# Modules
from src.modules.regex_matcher import RegexMatcher
from src.modules.result_aggregator import ResultAggregator

# Database
from src.database.connection import db_manager
from src.database import crud

def run_process_3(config: dict, context: dict):
    """
    [Process 3] ì •ê·œí‘œí˜„ì‹(Regex) ë§¤ì¹­ ê²€ì¦ í”„ë¡œì„¸ìŠ¤
    
    - ê³µí†µ: RegexMatcherë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ì „ì²´ì—ì„œ PII íƒì§€ ìˆ˜í–‰
    - Train ëª¨ë“œ: BIO íƒœê·¸ë¥¼ íŒŒì‹±í•œ 'ì •ë‹µ ë‹¨ì–´'ì™€ ì •ê·œì‹ íƒì§€ ê²°ê³¼ë¥¼ 1:1 ë¹„êµ (ì •íƒ/ì˜¤íƒ/ë¯¸íƒ)
    - Test ëª¨ë“œ: ì •ë‹µ ì—†ì´ ì •ê·œì‹ìœ¼ë¡œ íƒì§€ëœ ê²°ê³¼ë¥¼ ëª¨ë‘ ì €ì¥
    """
    
    # ==============================================================================
    # [Step 1] ì„¤ì • ë° ë¡œê±° ì´ˆê¸°í™”
    # ==============================================================================
    exp_conf = config['experiment']
    train_conf = config['train']
    
    experiment_code = exp_conf['experiment_code']
    data_category = exp_conf.get('data_category', 'personal_data')
    run_mode = exp_conf.get('run_mode', 'train')
    
    logger = logging.getLogger(experiment_code)
    logger.info(f"ğŸš€ [Process 3] Start Regex Matching Verification (Mode: {run_mode})")

    # ==============================================================================
    # [Step 2] ë°ì´í„° ë° ë„êµ¬ ì¤€ë¹„
    # ==============================================================================
    valid_loader = context['valid_loader']
    preprocessor = context['preprocessor']
    tokenizer = preprocessor.tokenizer
    
    # BIO ë¼ë²¨ ë§µ (ID <-> Name) {0: "O", 1: "B-ê°œì¸ì •ë³´", ...}
    # Train ëª¨ë“œì—ì„œ ì •ë‹µ(GT) íŒŒì‹±ì„ ìœ„í•´ í•„ìš”
    ner_id2label = preprocessor.ner_id2label 

    # RegexMatcher ì´ˆê¸°í™” (ë‚´ë¶€ì ìœ¼ë¡œ Detectors ë¡œë“œ)
    matcher = RegexMatcher()

    # ==============================================================================
    # [Step 3] ë§¤ì¹­ ë° ê²€ì¦ ë£¨í”„
    # ==============================================================================
    aggregator = ResultAggregator()
    start_time = datetime.now()
    process_epoch = 1

    logger.info("Starting regex detection loop...")
    
    for batch in tqdm(valid_loader, desc="Regex Matching"):
        batch_size = len(batch['sentence'])
        
        # Tensor -> List ë³€í™˜
        input_ids_batch = batch['input_ids'].cpu().tolist()
        labels_batch = batch['labels'].cpu().tolist()

        for i in range(batch_size):
            # 3-1. ë©”íƒ€ ë°ì´í„° ì¶”ì¶œ
            sentence_id = batch['sentence_id'][i]
            original_sentence = batch['sentence'][i]
            file_name = batch['file_name'][i]
            domain_id = batch['domain_id'][i]
            sentence_seq = batch['sentence_seq'][i]
            
            seq_val = sentence_seq.item() if hasattr(sentence_seq, 'item') else sentence_seq

            # 3-2. Regex íƒì§€ ìˆ˜í–‰ (ë¬¸ì¥ ì „ì²´ ìŠ¤ìº”)
            # results = [{'match': '010-1234-5678', 'label': 'ì „í™”ë²ˆí˜¸', ...}, ...]
            regex_results = matcher.detect(original_sentence)
            
            # 3-3. Regex ê²°ê³¼ë¥¼ í”„ë¡œì íŠ¸ ë¼ë²¨(ê°œì¸ì •ë³´/ê¸°ë°€ì •ë³´)ë¡œ ë§¤í•‘ ë° í•„í„°ë§
            # pred_spans = { "010-1234-5678": "ê°œì¸ì •ë³´" }
            pred_spans = {}
            for res in regex_results:
                raw_label = res['label'] # ì˜ˆ: "ì „í™”ë²ˆí˜¸"
                type_info = matcher.DETECTOR_TYPE_MAP.get(raw_label, {})
                
                # í˜„ì¬ ì‹¤í—˜ì˜ data_categoryì— ë§ëŠ” ê²ƒë§Œ ë‚¨ê¹€
                target_label = None
                if data_category == "personal_data" and type_info.get("category") == "ê°œì¸":
                    target_label = "ê°œì¸ì •ë³´" 
                elif data_category == "confidential_data" and type_info.get("category") == "ê¸°ë°€":
                    target_label = "ê¸°ë°€ì •ë³´"
                
                if target_label:
                    pred_spans[res['match']] = target_label

            # -----------------------------------------------------------
            # [Case A] Train Mode (GTì™€ ë¹„êµí•˜ì—¬ ì •ë°€ ê²€ì¦)
            # -----------------------------------------------------------
            if run_mode == 'train':
                # (1) BIO íƒœê·¸ íŒŒì‹±í•˜ì—¬ ì •ë‹µ(GT) ë‹¨ì–´ ì¶”ì¶œ
                current_input_ids = input_ids_batch[i]
                current_tags = labels_batch[i]
                tokens = tokenizer.convert_ids_to_tokens(current_input_ids)
                
                gt_entities = _extract_entities_from_bio(tokens, current_tags, ner_id2label, tokenizer)
                
                # íƒ€ê²Ÿ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” GTë§Œ í•„í„°ë§ (ì˜ˆ: 'ê°œì¸ì •ë³´'ë§Œ)
                # configì— ì •ì˜ëœ íƒ€ê²Ÿ ë¼ë²¨ëª… ì‚¬ìš©
                expected_label_name = "ê°œì¸ì •ë³´" if data_category == "personal_data" else "ê¸°ë°€ì •ë³´"
                
                target_gt_spans = {
                    word: label for word, label in gt_entities.items()
                    if label == expected_label_name
                }

                # (2) ì •íƒ/ì˜¤íƒ/ë¯¸íƒ ë¶„ë¥˜ (Set ì—°ì‚°)
                pred_words = set(pred_spans.keys())
                gt_words = set(target_gt_spans.keys())

                hits = pred_words & gt_words       # êµì§‘í•©
                wrongs = pred_words - gt_words     # ì˜ˆì¸¡ì—” ìˆëŠ”ë° ì •ë‹µì—” ì—†ìŒ
                mismatches = gt_words - pred_words # ì •ë‹µì—” ìˆëŠ”ë° ì˜ˆì¸¡ì—” ì—†ìŒ

                # (3) ë¡œê·¸ ê¸°ë¡
                for word in hits:
                    _add_log(aggregator, "hit", sentence_id, file_name, seq_val, 
                             original_sentence, word, domain_id, 
                             expected_label_name, expected_label_name, experiment_code, process_epoch)
                
                for word in wrongs:
                    _add_log(aggregator, "wrong", sentence_id, file_name, seq_val, 
                             original_sentence, word, domain_id, 
                             "O", expected_label_name, experiment_code, process_epoch)

                for word in mismatches:
                    _add_log(aggregator, "mismatch", sentence_id, file_name, seq_val,
                             original_sentence, word, domain_id,
                             expected_label_name, "O", experiment_code, process_epoch)

            # -----------------------------------------------------------
            # [Case B] Test Mode (ë‹¨ìˆœ íƒì§€ ê²°ê³¼ ì €ì¥)
            # -----------------------------------------------------------
            elif run_mode == 'test':
                # ë¹„êµí•  GTê°€ ì—†ìœ¼ë¯€ë¡œ íƒì§€ëœ ëª¨ë“  ê²ƒì„ 'hit' (ë˜ëŠ” prediction)ìœ¼ë¡œ ì²˜ë¦¬
                for word, label in pred_spans.items():
                    _add_log(aggregator, "hit", sentence_id, file_name, seq_val,
                             original_sentence, word, domain_id,
                             "Unknown", label, experiment_code, process_epoch)

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    # ==============================================================================
    # [Step 4] DB ì €ì¥
    # ==============================================================================
    with db_manager.get_db() as session:
        # 4-1. ë¬¸ì¥ ë¡œê·¸ ì €ì¥ (Bulk Insert)
        total_logs = 0
        for r_type in ["hit", "wrong", "mismatch"]:
            logs = aggregator.get_logs(r_type)
            if logs:
                crud.bulk_insert_inference_sentences(session, logs)
                total_logs += len(logs)
        
        logger.info(f"Saved {total_logs} inference logs to DB.")

        # 4-2. í”„ë¡œì„¸ìŠ¤ ê²°ê³¼ ìš”ì•½ ì €ì¥
        process_results = {
            "metrics": aggregator.get_metrics(),
            "detected_count": total_logs,
            "run_mode": run_mode
        }
        
        crud.create_process_result(session, {
            "experiment_code": experiment_code,
            "process_code": "process_3",
            "process_epoch": process_epoch,
            "process_start_time": start_time,
            "process_end_time": end_time,
            "process_duration": duration,
            "process_results": process_results
        })
        
    logger.info("[Process 3] Completed Successfully.")
    return context


# ------------------------------------------------------------------------------
# Helper Functions
# ------------------------------------------------------------------------------

def _extract_entities_from_bio(tokens, tags, id2label, tokenizer):
    """
    BIO íƒœê·¸ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ {ë‹¨ì–´: ë¼ë²¨} ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
    """
    entities = {}
    current_tokens = []
    current_label = None
    
    for token, tag_id in zip(tokens, tags):
        if tag_id == -100: continue
        label_name = id2label.get(tag_id, "O")
        
        if label_name.startswith("B-"):
            if current_tokens:
                word = tokenizer.convert_tokens_to_string(current_tokens)
                entities[word] = current_label
            current_tokens = [token]
            current_label = label_name[2:]
            
        elif label_name.startswith("I-") and current_label == label_name[2:]:
            current_tokens.append(token)
            
        else: # "O"
            if current_tokens:
                word = tokenizer.convert_tokens_to_string(current_tokens)
                entities[word] = current_label
            current_tokens = []
            current_label = None
            
    if current_tokens:
        word = tokenizer.convert_tokens_to_string(current_tokens)
        entities[word] = current_label
        
    return entities

def _add_log(aggregator, match_type, sent_id, fname, seq, origin_sent, word, domain, gt, pred, exp_code, epoch=1):
    """ë¡œê·¸ ë°ì´í„° ìƒì„± ë° ì§‘ê³„ê¸°ì— ì¶”ê°€"""
    sentence_inference_result = {
        "sentence_id": sent_id,
        "source_file_name": fname,
        "sequence_in_file": seq,
        "origin_sentence": origin_sent,
        "domain_id": domain,
        "inference_results": [{
            "word": word,
            "label": pred,
            "match_result": match_type,
            "ground_truth": gt
        }],
        # entity_countëŠ” ì´ ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ì™€ ê°™ìŒ (ì—¬ê¸°ì„  ë‹¨ì–´ ë‹¨ìœ„ ë¡œê·¸ë¼ 1)
        "entity_count": 1 
    }
    
    log_entry = {
        "experiment_code": exp_code,
        "process_code": "process_3",
        "process_epoch": epoch,
        "sentence_id": sent_id,
        "sentence_inference_result": sentence_inference_result,
        "confidence_score": 1.0
    }
    
    # í†µê³„ìš© ID (ì„ì˜ê°’ 0)
    aggregator.add_result(match_type, log_entry, 0)