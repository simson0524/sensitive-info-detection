# src/processes/process_0.py

import torch
import os
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split

# Modules: Nerëª¨ë¸ê³¼ í•´ë‹¹ ëª¨ë¸ì— ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê¸° ìœ„í•œ ì „ì²˜ë¦¬ ëª¨ë“ˆ
from src.modules.ner_preprocessor import NerPreprocessor
from src.models.ner_roberta import RobertaNerModel

# Utils: ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
from src.utils.common import set_seed
from src.utils.logger import setup_experiment_logger

def run_process_0(config: dict) -> dict:
    """
    [Process 0] í•™ìŠµ í™˜ê²½ ë° ê°ì²´ ì´ˆê¸°í™” í”„ë¡œì„¸ìŠ¤ (Setup Phase)
    
    ì´ í•¨ìˆ˜ì˜ ì—­í• :
    1. ë°ì´í„° ë¡œë“œ: í•˜ë‚˜ì˜ ì›ë³¸ í´ë”ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ Train/Validë¡œ ìë™ ë¶„í• í•©ë‹ˆë‹¤.
    2. ëª¨ë¸ ì´ˆê¸°í™”: ì„¤ì •ëœ ì•„í‚¤í…ì²˜(RoBERTa ë“±)ë¡œ ëª¨ë¸ ê»ë°ê¸°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    3. ê°€ì¤‘ì¹˜ ë¡œë“œ: ë§Œì•½ ì´ì–´ì„œ í•™ìŠµ(Resume)í•´ì•¼ í•œë‹¤ë©´ ì €ì¥ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    4. ë„êµ¬ ì¤€ë¹„: Optimizer, Scheduler ë“±ì„ ì¤€ë¹„í•˜ì—¬ íŒ¨í‚¤ì§•(Context)í•©ë‹ˆë‹¤.
    
    Args:
        config (dict): experiment_config.yamlì—ì„œ ë¡œë“œí•œ ì„¤ì •ê°’
        
    Returns:
        dict: í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ê°ì²´ê°€ ë‹´ê¸´ Context
    """
    
    # ==============================================================================
    # [Step 1] ì„¤ì • ë¡œë“œ ë° ë¡œê±° ì´ˆê¸°í™”
    # ==============================================================================
    exp_conf = config['experiment']
    train_conf = config['train']
    path_conf = config['path']
    experiment_code = exp_conf['experiment_code']
    run_mode = exp_conf.get('run_mode', 'train') # ë¬´ì¡°ê±´ 'train' or 'test'
    
    # ë¡œê±° ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ íŒŒì¼ê³¼ í•¨ê»˜ ìƒì„±)
    logger = setup_experiment_logger(experiment_code, path_conf['log_dir'])
    logger.info(f"ğŸ› ï¸ [Process 0] Initializing Experiment: {experiment_code}")

    # ì¬í˜„ì„±ì„ ìœ„í•´ ëœë¤ ì‹œë“œ ê³ ì • (ë°ì´í„° ë¶„í•  ê²°ê³¼ê°€ ë§¤ë²ˆ ê°™ì•„ì•¼ í•¨)
    set_seed(train_conf.get('seed', 42))
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


    # ==============================================================================
    # [Step 2] ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë“œ (Data Preparation)
    # ==============================================================================
    logger.info("Step 1: Loading Data & Splitting Train/Valid...")
    
    # HuggingFace Tokenizer ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(train_conf['model_name'])
    
    # 2-1. ì „ì²˜ë¦¬ê¸°(Preprocessor) ì´ˆê¸°í™”
    # ì´ ì¹œêµ¬ê°€ JSON ë¡œë“œ, BIO íƒœê¹… ë³€í™˜ ë“±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    preprocessor = NerPreprocessor(
        tokenizer=tokenizer, 
        max_len=train_conf['max_len'], 
        label2id=train_conf['label_map']
    )
    
    # 2-2. ì „ì²´ Raw Data ë¡œë“œ
    # ì§€ì •ëœ í´ë”(path_conf['data_dir']) ë‚´ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.
    all_samples, all_annos = preprocessor.load_data(path_conf['data_dir'])
    
    total_count = len(all_samples)
    if total_count == 0:
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë” ì´ìƒ ì§„í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì—ëŸ¬ ë°œìƒ
        raise ValueError(f"âŒ No data found in {path_conf['data_dir']}")

    # 2-3. Train / Valid ìë™ ë¶„í•  (sklearn ì‚¬ìš©)
    # ë³„ë„ì˜ ê²€ì¦ í´ë”ë¥¼ ë‘ì§€ ì•Šê³ , ì „ì²´ ë°ì´í„°ì—ì„œ ì¼ì • ë¹„ìœ¨ì„ ë–¼ì–´ë‚´ì–´ ê²€ì¦ìš©ìœ¼ë¡œ ì”ë‹ˆë‹¤.
    val_ratio = train_conf.get('validation_split', 0.2) # ê¸°ë³¸ê°’ 20%
    all_ids = list(all_samples.keys())
    
    # ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ì–´ì„œ ë‚˜ëˆ•ë‹ˆë‹¤. (random_stateê°€ ê³ ì •ë˜ì–´ ìˆì–´ ë§¤ë²ˆ ê²°ê³¼ê°€ ê°™ìŒ)
    train_ids, valid_ids = train_test_split(
        all_ids, 
        test_size=val_ratio, 
        random_state=train_conf.get('seed', 42), 
        shuffle=True
    )
    
    # IDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹¤ì œ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì¶”ì¶œí•˜ì—¬ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
    train_samples = {uid: all_samples[uid] for uid in train_ids}
    train_annos = {uid: all_annos[uid] for uid in train_ids}
    
    valid_samples = {uid: all_samples[uid] for uid in valid_ids}
    valid_annos = {uid: all_annos[uid] for uid in valid_ids}

    logger.info(f"ğŸ“Š Data Split Result: Total({total_count}) -> Train({len(train_ids)}) / Valid({len(valid_ids)})")

    # 2-4. Dataset ê°ì²´ ìƒì„± (ì‹¤ì œ í† í°í™” ë° BIO íƒœê¹… ìˆ˜í–‰)
    # ê°œì¸ì •ë³´/ê¸°ë°€ì •ë³´ ì—¬ë¶€ì— ë”°ë¼ í•„í„°ë§ ì˜µì…˜(data_category)ì„ ì ìš©í•©ë‹ˆë‹¤.
    data_category = exp_conf.get('data_category', 'personal_data')
    
    logger.info("Creating Train Dataset...")
    train_dataset = preprocessor.create_dataset(train_samples, train_annos, data_category=data_category)
    
    logger.info("Creating Valid Dataset...")
    valid_dataset = preprocessor.create_dataset(valid_samples, valid_annos, data_category=data_category)
    
    # 2-5. DataLoader ìƒì„± (Batch ë‹¨ìœ„ ê³µê¸‰ê¸°)
    train_loader = DataLoader(train_dataset, batch_size=train_conf['batch_size'], shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=train_conf['batch_size'], shuffle=False)


    # ==============================================================================
    # [Step 3] ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ë¡œë“œ (Model Setup)
    # ==============================================================================
    logger.info("Step 2: Building Model & Optimizer...")
    
    # ê¸°ë³¸ Encoder (RoBERTa) ë¡œë“œ
    encoder = AutoModel.from_pretrained(train_conf['model_name'])
    num_labels = len(preprocessor.ner_label2id) # BIO íƒœê·¸ ê°œìˆ˜ ìë™ ê³„ì‚°
    
    # ìš°ë¦¬ê°€ ì •ì˜í•œ Custom NER ëª¨ë¸ ìƒì„±
    model = RobertaNerModel(
        encoder=encoder,
        num_classes=num_labels,
        use_focal=train_conf.get('use_focal', False) # Focal Loss ì‚¬ìš© ì—¬ë¶€
    ).to(device)

    # --------------------------------------------------------------------------
    # [ì¤‘ìš”] í•™ìŠµ ì¬ê°œ (Resume Training) ë¡œì§
    # configì— 'resume_checkpoint' ê²½ë¡œê°€ ìˆê³ , íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ë©´ ê°€ì¤‘ì¹˜ë¥¼ ë®ì–´ì”Œì›ë‹ˆë‹¤.
    # --------------------------------------------------------------------------
    target_ckpt_path = None
    
    if run_mode == 'test':
        # Test ëª¨ë“œ: inference_checkpoint ë¡œë“œ (í•„ìˆ˜)
        target_ckpt_path = path_conf.get('inference_checkpoint')
        if not target_ckpt_path:
            logger.warning("âš ï¸ [Test Mode] 'inference_checkpoint' is not set in config!")
    else:
        # Train ëª¨ë“œ: resume_checkpoint ë¡œë“œ (ì„ íƒ)
        target_ckpt_path = path_conf.get('resume_checkpoint')

    # ê²½ë¡œê°€ ì¡´ì¬í•˜ë©´ ë¡œë“œ ìˆ˜í–‰
    if target_ckpt_path and os.path.exists(target_ckpt_path):
        logger.info(f"ğŸ“¥ Loading Weights from: {target_ckpt_path}")
        try:
            state_dict = torch.load(target_ckpt_path, map_location=device)
            model.load_state_dict(state_dict)
            logger.info("âœ… Weights loaded successfully.")
        except Exception as e:
            logger.error(f"âŒ Failed to load checkpoint: {e}")
            raise e
    else:
        if run_mode == 'test':
            logger.warning("âš ï¸ Running TEST mode with RANDOM weights (Checkpoint not found).")
        else:
            logger.info("ğŸ†• Initialized model with Base Weights (No resume checkpoint found).")

    # ==============================================================================
    # [Step 4] í•™ìŠµ ë„êµ¬ ì„¤ì • (Optimizer & Scheduler)
    # ==============================================================================
    optimizer = AdamW(model.parameters(), lr=float(train_conf['learning_rate']))
    
    total_steps = len(train_loader) * train_conf['epochs']
    scheduler = get_linear_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=int(total_steps * 0.1), # ì „ì²´ ìŠ¤í…ì˜ 10% ë™ì•ˆ Warmup
        num_training_steps=total_steps
    )

    logger.info("âœ… [Process 0] Setup Completed Successfully.")

    # ==============================================================================
    # [Step 5] Context íŒ¨í‚¤ì§• ë° ë°˜í™˜
    # ==============================================================================
    # ë‹¤ìŒ í”„ë¡œì„¸ìŠ¤(Process 1, 2...)ì—ì„œ ì‚¬ìš©í•  ê°ì²´ë“¤ì„ ë”•ì…”ë„ˆë¦¬ì— ë‹´ì•„ ë³´ëƒ…ë‹ˆë‹¤.
    context = {
        "experiment_code": experiment_code,
        "device": device,
        "model": model,
        "optimizer": optimizer,
        "scheduler": scheduler,
        "train_loader": train_loader,
        "valid_loader": valid_loader,
        
        # Preprocessor ê°ì²´ (í† í¬ë‚˜ì´ì €, ë¼ë²¨ë§µ í¬í•¨)ëŠ” í›„ì† í”„ë¡œì„¸ìŠ¤ì—ì„œë„ ê³„ì† í•„ìš”í•¨
        "preprocessor": preprocessor, 
        
        # Dataset ê°ì²´ (ìƒíƒœ ìœ ì§€ìš©, Process 4 ë“±ì—ì„œ ì¬í™œìš©)
        "train_dataset": train_dataset, 
        "valid_dataset": valid_dataset
    }
    
    return context