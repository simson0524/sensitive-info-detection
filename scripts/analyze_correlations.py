# temp

import os
import json
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, spearmanr, kendalltau
from src.database.connection import db_manager
from src.database.models import DomainTermMatrix, Domain
from datetime import datetime
from tqdm import tqdm

def analyze_domain_correlations():
    # 1. ê²½ë¡œ ì„¤ì •
    output_dir = "outputs/logs"
    meta_path = "/home/student1/sensitive-info-detection/src/modules/new_domain_generation_metadata/domain_form_history.json"
    os.makedirs(output_dir, exist_ok=True)
    
    summary_file = os.path.join(output_dir, "correlation_summary_report.txt")
    detail_csv = os.path.join(output_dir, "correlation_details.csv")

    # 2. JSON ë©”íƒ€ë°ì´í„° ë¡œë“œ
    print("ğŸ“– ë©”íƒ€ë°ì´í„°(JSON)ë¥¼ ì½ì–´ì˜¤ëŠ” ì¤‘...")
    try:
        with open(meta_path, 'r', encoding='utf-8') as f:
            meta_data = json.load(f)
        
        domain_meta = {}
        for d_id_str, info in meta_data.get("domain_form", {}).items():
            d_id = int(d_id_str)
            if d_id == 999: continue  # ë©”íƒ€ë°ì´í„°ì—ì„œë„ 999ë²ˆ ì œì™¸
            
            formatted_id = f"ID:{d_id_str.zfill(3)}" 
            domain_meta[d_id] = {
                "id_label": formatted_id,
                "kor_name": info.get("domain_name", "ì•Œìˆ˜ì—†ìŒ"),
                "eng_name": info.get("domain_title", "Unknown")
            }
    except Exception as e:
        print(f"âŒ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 3. DB ë°ì´í„° ë¡œë“œ (999ë²ˆ ì œì™¸ ì¿¼ë¦¬)
    print("ğŸ“¦ DBì—ì„œ Z-score ë°ì´í„°ë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤ (999ë²ˆ ì œì™¸)...")
    with db_manager.get_db() as session:
        # ë„ë©”ì¸ ëª©ë¡ ê°€ì ¸ì˜¬ ë•Œ 999 ì œì™¸
        domain_ids = [d[0] for d in session.query(Domain.domain_id).filter(Domain.domain_id != 999).all()]
        domain_ids = sorted(domain_ids)
        
        # í–‰ë ¬ ë°ì´í„° ê°€ì ¸ì˜¬ ë•Œ 999 ì œì™¸
        query = session.query(
            DomainTermMatrix.domain_id, 
            DomainTermMatrix.term, 
            DomainTermMatrix.z_score
        ).filter(DomainTermMatrix.domain_id != 999)
        df_all = pd.read_sql(query.statement, session.bind)

    # 4. ë³€ìˆ˜ ì´ˆê¸°í™”
    n = len(domain_ids)
    methods = ['pearson', 'spearman', 'kendall']
    corr_matrices = {m: pd.DataFrame(index=domain_ids, columns=domain_ids, dtype=float) for m in methods}
    for m in methods: np.fill_diagonal(corr_matrices[m].values, 1.0)

    detail_records = []
    pairs = [(domain_ids[i], domain_ids[j]) for i in range(n) for j in range(i + 1, n)]

    # 5. ìƒê´€ê´€ê³„ ì—°ì‚°
    for id_a, id_b in tqdm(pairs, desc="ìƒê´€ê´€ê³„ ì—°ì‚° ì¤‘"):
        meta_a = domain_meta.get(id_a, {"kor_name": f"ë„ë©”ì¸{id_a}", "id_label": f"ID:{id_a}"})
        meta_b = domain_meta.get(id_b, {"kor_name": f"ë„ë©”ì¸{id_b}", "id_label": f"ID:{id_b}"})
        
        data_a = df_all[df_all['domain_id'] == id_a][['term', 'z_score']]
        data_b = df_all[df_all['domain_id'] == id_b][['term', 'z_score']]
        
        merged = pd.merge(data_a, data_b, on='term', suffixes=('_a', '_b'))
        common_count = len(merged)

        res = {
            "id_a": id_a, "id_b": id_b,
            "domain_a_kor": meta_a['kor_name'], "domain_b_kor": meta_b['kor_name'],
            "common_terms": common_count,
            "pearson": np.nan, "spearman": np.nan, "kendall": np.nan
        }

        if common_count > 2:
            va, vb = merged['z_score_a'], merged['z_score_b']
            res["pearson"], _ = pearsonr(va, vb)
            res["spearman"], _ = spearmanr(va, vb)
            res["kendall"], _ = kendalltau(va, vb)
            for m in methods:
                corr_matrices[m].at[id_a, id_b] = corr_matrices[m].at[id_b, id_a] = res[m]

        detail_records.append(res)

    # 6. ìƒì„¸ ê²°ê³¼ CSV ì €ì¥
    pd.DataFrame(detail_records).to_csv(detail_csv, index=False, encoding='utf-8-sig')

    # 7. êµìˆ˜ë‹˜ ë³´ê³ ìš© ìš”ì•½ TXT ì‘ì„±
    with open(summary_file, "w", encoding="utf-8") as f:
        f.write("ğŸ” ë„ë©”ì¸ë³„ Z-score ìƒê´€ê´€ê³„ ë¶„ì„ ë³´ê³ ì„œ (ì œì™¸ ë„ë©”ì¸: 999)\n")
        f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ìµœì¢… ë¶„ì„ ë„ë©”ì¸ ìˆ˜: {n}ê°œ\n")
        f.write("-" * 60 + "\n\n")

        df_detail = pd.DataFrame(detail_records)
        for m in methods:
            avg_val = df_detail[m].dropna().mean()
            f.write(f"ğŸ“Š {m.capitalize()} í‰ê·  ìƒê´€ê³„ìˆ˜: {avg_val:.4f}\n")

        f.write("\n[TOP 10 ìœ ì‚¬ ë„ë©”ì¸ ìŒ (Pearson ê¸°ì¤€)]\n")
        top_10 = df_detail.sort_values(by="pearson", ascending=False).head(10)
        for i, (_, row) in enumerate(top_10.iterrows(), 1):
            f.write(f"{i}. {row['domain_a_kor']} - {row['domain_b_kor']}: {row['pearson']:.4f} (ê³µí†µë‹¨ì–´ {row['common_terms']}ê°œ)\n")

    # 8. íˆíŠ¸ë§µ ìƒì„± (ID:XX í˜•ì‹)
    print("ğŸ¨ íˆíŠ¸ë§µ ìƒì„± ì¤‘...")
    for m in methods:
        plt.figure(figsize=(24, 20))
        plot_df = corr_matrices[m].copy()
        
        labels = [domain_meta.get(idx, {"id_label": str(idx)})["id_label"] for idx in plot_df.index]
        plot_df.index = labels
        plot_df.columns = labels
        
        sns.heatmap(plot_df.astype(float), annot=False, cmap='RdBu_r', center=0)
        plt.title(f"Domain Correlation Matrix ({m.capitalize()}) - Excl. 999", fontsize=22)
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f"heatmap_{m}.png"))
        plt.close()

    print(f"âœ¨ ë¶„ì„ ì™„ë£Œ! 999ë²ˆ ë„ë©”ì¸ì„ ì œì™¸í•˜ê³  ì´ {n}ê°œ ë„ë©”ì¸ì— ëŒ€í•œ ê²°ê³¼ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    analyze_domain_correlations()