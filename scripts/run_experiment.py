# scripts/run_experiment.py

import sys
import os
import json
from datetime import datetime
import traceback

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ pathì— ì¶”ê°€ (src ëª¨ë“ˆ ì¸ì‹ì„ ìœ„í•´)
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

# Utils
from src.utils.common import load_yaml
from src.utils.logger import setup_experiment_logger

# Database
from src.database.connection import db_manager
from src.database import crud

# Processes
from src.processes.process_0 import run_process_0
from src.processes.process_1 import run_process_1
from src.processes.process_2 import run_process_2
from src.processes.process_3 import run_process_3
from src.processes.process_4 import run_process_4

def main():
    """
    [Main Script] ì‹¤í—˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì§€íœ˜ (Orchestrator)
    
    1. ì„¤ì • ë¡œë“œ ë° DB ì‹¤í—˜ ë“±ë¡
    2. Process 0: ì¤€ë¹„ (ë°ì´í„°ì…‹, ëª¨ë¸, ê°€ì¤‘ì¹˜ ë¡œë“œ)
    3. Process 1~4: ëª¨ë“œ(Train/Test)ì— ë”°ë¼ ìˆœì°¨ì  ì‹¤í–‰
    4. ì¢…ë£Œ ì²˜ë¦¬ ë° ìƒì„¸ ê²°ê³¼ ë¦¬í¬íŠ¸(TXT) ìƒì„±
    """
    
    # --------------------------------------------------------------------------
    # [Step 1] ì„¤ì • ë¡œë“œ(base & exp config merge load) ë° ì´ˆê¸°í™”
    # --------------------------------------------------------------------------
    base_conf_path = os.path.join(project_root, "configs", "base_config.yaml")
    exp_conf_path = os.path.join(project_root, "configs", "experiment_config.yaml")
    
    # ë‘ ì„¤ì • íŒŒì¼ì„ ë¡œë“œ í›„ ë³‘í•©
    base_config = load_yaml(base_conf_path)
    exp_config = load_yaml(exp_conf_path)

    config = base_config.copy()
    
    for section, values in exp_config.items():
        if section in config and isinstance(config[section], dict) and isinstance(values, dict):
            # í•˜ìœ„ í‚¤(train, path ë“±) ì—…ë°ì´íŠ¸
            config[section].update(values)
        else:
            # ìƒˆë¡œìš´ ì„¹ì…˜ì´ë©´ í†µì§¸ë¡œ ì¶”ê°€
            config[section] = values

    exp_conf = config['experiment']
    path_conf = config['path']
    
    experiment_code = exp_conf['experiment_code']
    run_mode = exp_conf.get('run_mode', 'train') # 'train' or 'test'
    
    # ì „ì—­ ë¡œê±° ì„¤ì •
    logger = setup_experiment_logger(experiment_code, path_conf['log_dir'])
    logger.info("="*60)
    logger.info(f"ðŸŽ¬ Experiment Started: {experiment_code} (Mode: {run_mode.upper()})")
    logger.info("="*60)

    try:
        # --------------------------------------------------------------------------
        # [Step 2] DBì— ì‹¤í—˜ ì •ë³´ ë“±ë¡ (Experiment Table)
        # --------------------------------------------------------------------------
        logger.info("Step 0: Registering Experiment to DB...")
        
        with db_manager.get_db() as session:
            # ì‹¤í—˜ ì •ë³´ê°€ ì´ë¯¸ ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸
            existing_exp = crud.get_experiment(session, experiment_code)
            
            if existing_exp:
                logger.warning(f"âš ï¸ Experiment {experiment_code} already exists. Updating start time...")
                crud.update_experiment(session, experiment_code, {
                    "experiment_start_time": datetime.now(),
                    "experiment_config": config,
                    "run_mode": run_mode 
                })
            else:
                # ì‹ ê·œ ì‹¤í—˜ ìƒì„±
                exp_data = {
                    "experiment_code": experiment_code,
                    "previous_experiment_code": exp_conf.get('previous_experiment_code'),
                    "data_category": exp_conf.get('data_category', 'personal_data'),
                    "run_mode": run_mode,
                    "experiment_config": config,
                    "dataset_absolute_path": path_conf.get('data_dir'),
                    "experiment_start_time": datetime.now(),
                    "experiment_duration": 0.0,
                    "dataset_info": {} 
                }
                crud.create_experiment(session, exp_data)
                logger.info("âœ… Experiment record created.")

        # --------------------------------------------------------------------------
        # [Step 3] í”„ë¡œì„¸ìŠ¤ ìˆœì°¨ ì‹¤í–‰
        # --------------------------------------------------------------------------
        
        # [Process 0] ì¤€ë¹„ ë‹¨ê³„
        context = run_process_0(config)
        
        # [Process 1] Run Modeì— ë”°ë¥¸ ì‹¤í–‰ íë¦„ ì œì–´
        if run_mode == "train":
            if exp_conf.get('run_process_1', True):
                logger.info("â–¶ï¸ Running Process 1 (Training)...")
                context = run_process_1(config, context)
            else:
                logger.info("â­ï¸ Process 1 skipped by config.")
                
        elif run_mode == "test":
            logger.info("â­ï¸ Skipping Process 1 (Training) due to TEST mode.")
        
        # [Process 2] ì‚¬ì „ ë§¤ì¹­ ê²€ì¦
        if exp_conf.get('run_process_2', True):
            logger.info("â–¶ï¸ Running Process 2 (Dictionary Matching)...")
            run_process_2(config, context)
            
        # [Process 3] ì •ê·œì‹ ë§¤ì¹­ ê²€ì¦
        if exp_conf.get('run_process_3', True):
            logger.info("â–¶ï¸ Running Process 3 (Regex Matching)...")
            run_process_3(config, context)
            
        # [Process 4] ëª¨ë¸ ë³´ì™„ ì¶”ë¡  (Hybrid Logic)
        if exp_conf.get('run_process_4', True):
            logger.info("â–¶ï¸ Running Process 4 (Model Complementary Inference)...")
            run_process_4(config, context)

        # --------------------------------------------------------------------------
        # [Step 4] ì‹¤í—˜ ì¢…ë£Œ ì²˜ë¦¬ ë° ë¦¬í¬íŠ¸ ìƒì„±
        # --------------------------------------------------------------------------
        with db_manager.get_db() as session:
            end_time = datetime.now()
            exp_obj = crud.get_experiment(session, experiment_code)
            start_time = exp_obj.get('experiment_start_time') if exp_obj else end_time
            
            # crud.get_experimentê°€ dictë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ .get() ì‚¬ìš© (ì´ì „ row_to_dict ì ìš©ë¨)
            # ë§Œì•½ datetime ê°ì²´ë¼ë©´ ë°”ë¡œ ì‚¬ìš©
            
            if isinstance(start_time, datetime):
                # start_timeì´ timezone ì •ë³´(Aware)ë¥¼ ê°€ì§€ê³  ìžˆë‹¤ë©´ ì œê±°í•˜ì—¬ Naiveë¡œ ë³€í™˜
                # end_timeì€ datetime.now()ë¡œ ìƒì„±ë˜ì–´ ê¸°ë³¸ì ìœ¼ë¡œ Naive ìƒíƒœìž„
                if start_time.tzinfo is not None:
                    start_time = start_time.replace(tzinfo=None)
                
                duration = (end_time - start_time).total_seconds()

            # DB ì—…ë°ì´íŠ¸
            crud.update_experiment(session, experiment_code, {
                "experiment_end_time": end_time,
                "experiment_duration": duration
            })
            
            # [NEW] ìƒì„¸ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± (TXT)
            generate_experiment_report(session, experiment_code, path_conf['log_dir'])
            
        logger.info("="*60)
        logger.info(f"ðŸ Experiment Finished Successfully. (Duration: {duration:.2f}s)")
        logger.info("="*60)

    except Exception as e:
        logger.error(f"âŒ Experiment Failed: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)


def generate_experiment_report(session, experiment_code: str, log_dir: str):
    """
    DBì—ì„œ ì‹¤í—˜ ì •ë³´ì™€ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ê²°ê³¼ë¥¼ ì¡°íšŒí•˜ì—¬ ìƒì„¸ ë¦¬í¬íŠ¸(TXT)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    outputs/logs/{code}/{code}_all_process_results.txt
    
    í¬í•¨ ë‚´ìš©:
    1. Experiment Table ì •ë³´ (ì„¤ì •, ì‹œê°„ ë“± ì „ì²´)
    2. Process 1: Epochë³„ ìƒì„¸ ì§€í‘œ (Confusion Matrix í¬í•¨) ë° Best Epoch (F1, Loss)
    3. Process 2~4: ê° í”„ë¡œì„¸ìŠ¤ ê²°ê³¼ ìš”ì•½
    """
    report_lines = []
    
    # ---------------------------------------------------------
    # 1. Experiment General Info (Table Dump)
    # ---------------------------------------------------------
    exp_data = crud.get_experiment(session, experiment_code) # returns dict
    
    report_lines.append("="*80)
    report_lines.append(f"ðŸ“Š EXPERIMENT REPORT: {experiment_code}")
    report_lines.append(f"Generated At: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("="*80 + "\n")
    
    report_lines.append("[1] General Information (Experiment Table)")
    report_lines.append("-" * 80)
    
    if exp_data:
        for key, value in exp_data.items():
            # Config ê°™ì€ í° JSONì€ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
            if key == 'experiment_config' or key == 'dataset_info':
                val_str = json.dumps(value, indent=4, ensure_ascii=False)
                report_lines.append(f"* {key}:\n{val_str}")
            else:
                report_lines.append(f"* {key}: {value}")
    else:
        report_lines.append("Error: Experiment data not found.")
    report_lines.append("\n")

    # ---------------------------------------------------------
    # 2. Process 1 (Model Training) Details
    # ---------------------------------------------------------
    # process_codeê°€ 'process_1' ë˜ëŠ” 'model_train' ì¸ ê²ƒ í•„í„°ë§
    all_results = crud.get_process_results(session, experiment_code)
    p1_results = [r for r in all_results if r['process_code'] in ['process_1', 'model_train']]
    
    if p1_results:
        report_lines.append("[2] Process 1: Model Training & Validation")
        report_lines.append("-" * 80)
        
        # Best Metric Tracking
        best_f1 = -1.0
        best_f1_epoch = -1
        min_valid_loss = float('inf')
        min_loss_epoch = -1

        # Epochë³„ ìƒì„¸ ê¸°ë¡
        for res in p1_results:
            epoch = res['process_epoch']
            metrics = res['process_results']
            
            # ì£¼ìš” ì§€í‘œ ì¶”ì¶œ
            train_loss = metrics.get('train_loss', 0.0)
            valid_loss = metrics.get('valid_loss', 0.0)
            valid_f1 = metrics.get('valid_f1', 0.0)
            valid_prec = metrics.get('valid_precision', 0.0)
            valid_rec = metrics.get('valid_recall', 0.0)
            conf_matrix = metrics.get('confusion_matrix') # 2D List

            # Best Update
            if valid_f1 > best_f1:
                best_f1 = valid_f1
                best_f1_epoch = epoch
            
            if valid_loss < min_valid_loss:
                min_valid_loss = valid_loss
                min_loss_epoch = epoch
            
            # Line Writing
            report_lines.append(f"Epoch {epoch:02d}")
            report_lines.append(f"  - Train Loss: {train_loss:.5f} | Valid Loss: {valid_loss:.5f}")
            report_lines.append(f"  - F1: {valid_f1:.5f} | Precision: {valid_prec:.5f} | Recall: {valid_rec:.5f}")
            
            if conf_matrix:
                # Confusion Matrix ì˜ˆì˜ê²Œ ì¶œë ¥
                cm_str = json.dumps(conf_matrix) # í•œ ì¤„ë¡œ ë³´ê±°ë‚˜
                # cm_str = json.dumps(conf_matrix, indent=2) # ì—¬ëŸ¬ ì¤„ë¡œ ë³´ê±°ë‚˜ (ì—¬ê¸°ì„  í•œì¤„)
                report_lines.append(f"  - Confusion Matrix: {cm_str}")
            report_lines.append("") # ë¹ˆ ì¤„

        # ìš”ì•½ ì •ë³´
        report_lines.append("-" * 40)
        report_lines.append("ðŸ† Process 1 Summary")
        report_lines.append(f"  - Best Model (Max F1): Epoch {best_f1_epoch} (F1: {best_f1:.5f})")
        report_lines.append(f"  - Best Model (Min Loss): Epoch {min_loss_epoch} (Loss: {min_valid_loss:.5f})")
        report_lines.append("-" * 80 + "\n")

    # ---------------------------------------------------------
    # 3. Other Processes (2, 3, 4) Results
    # ---------------------------------------------------------
    # ì´ í”„ë¡œì„¸ìŠ¤ë“¤ì€ ë³´í†µ 1íšŒì„± ì‹¤í–‰(Epoch 1)ì´ë¯€ë¡œ ê°„ë‹¨ížˆ ì¶œë ¥
    for proc_code in ["process_2", "process_3", "process_4"]:
        # ìµœì‹  ê²°ê³¼ 1ê°œë§Œ ê°€ì ¸ì˜´ (í˜¹ì‹œ ì—¬ëŸ¬ ë²ˆ ëŒë ¸ì„ ìˆ˜ ìžˆìœ¼ë‹ˆ)
        results = [r for r in all_results if r['process_code'] == proc_code]
        if results:
            last_result = results[-1] # ê°€ìž¥ ìµœê·¼ ê²ƒ
            
            proc_name = proc_code.upper().replace("_", " ")
            report_lines.append(f"[{proc_name[-1]}th Step] {proc_name} Results")
            report_lines.append("-" * 80)
            
            metrics = last_result['process_results']
            formatted_json = json.dumps(metrics, indent=4, ensure_ascii=False)
            report_lines.append(formatted_json)
            report_lines.append("\n")

    # ---------------------------------------------------------
    # 4. íŒŒì¼ ì €ìž¥
    # ---------------------------------------------------------
    save_path = os.path.join(log_dir, experiment_code, f"{experiment_code}_all_process_results.txt")
    
    try:
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(report_lines))
        print(f"ðŸ“„ Final Report generated: {save_path}")
    except Exception as e:
        print(f"âŒ Failed to write report: {e}")


if __name__ == "__main__":
    main()